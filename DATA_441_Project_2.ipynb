{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea878927-393d-4126-89e0-94698ecb9032",
   "metadata": {
    "id": "462gW7XBGAl8"
   },
   "source": [
    "Peter Schnizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8358df-ca9e-4967-83c2-78c1d4ec3fc3",
   "metadata": {
    "id": "ZlpnyL5HZIHK"
   },
   "source": [
    "# HW 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845c807-ac6d-4cbd-8e2a-643651ef8cd0",
   "metadata": {
    "id": "4ELT10y8ZIJ9"
   },
   "source": [
    " ### Task 1:\n",
    "Create your class that implements the Gradient Boosting concept, based on the locally weighted regression method (Lowess class), and that allows a user-prescribed number of boosting steps. The class you develop should have all the mainstream useful options, including “fit,” “is_fitted”,  and “predict,” methods.  Show applications with real data for regression, 10-fold cross-validations and compare the effect of different scalers, such as the “StandardScaler”, “MinMaxScaler”, and the “QuantileScaler”.  In the case of the “Concrete” data set, determine a choice of hyperparameters that yield lower MSEs for your method when compared to the eXtream Gradient Boosting library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069498e-9dd3-4028-a8ec-061abd362b5f",
   "metadata": {
    "id": "5uVJy4weZIM0"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4b6f9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0b1239f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Kernel\n",
    "def Gaussian(w):\n",
    "  return np.where(w>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*w**2))\n",
    "\n",
    "# Tricubic Kernel\n",
    "def Tricubic(w):\n",
    "  return np.where(w>1,0,70/81*(1-w**3)**3)\n",
    "\n",
    "# Quartic Kernel\n",
    "def Quartic(w):\n",
    "  return np.where(w>1,0,15/16*(1-w**2)**2)\n",
    "\n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(w):\n",
    "  return np.where(w>1,0,3/4*(1-w**2))\n",
    "\n",
    "def weight_function(u,v,kern=Gaussian,tau=0.5):\n",
    "    return kern(cdist(u, v, metric='euclidean')/(2*tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0acb8c40-3ea2-4995-b385-598af2705040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lowess:\n",
    "    def __init__(self, kernel = Gaussian, tau=0.05):\n",
    "        self.kernel = kernel\n",
    "        self.tau = tau\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        kernel = self.kernel\n",
    "        tau = self.tau\n",
    "        self.xtrain_ = x\n",
    "        self.yhat_ = y\n",
    "\n",
    "    def predict(self, x_new):\n",
    "        check_is_fitted(self)\n",
    "        x = self.xtrain_\n",
    "        y = self.yhat_\n",
    "        lm = linear_model.Ridge(alpha=0.0001)\n",
    "        w = weight_function(x,x_new,self.kernel,self.tau)\n",
    "\n",
    "        if np.isscalar(x_new):\n",
    "          lm.fit(np.diag(w)@(x.reshape(-1,1)),np.diag(w)@(y.reshape(-1,1)))\n",
    "          yest = lm.predict([[x_new]])[0][0]\n",
    "        else:\n",
    "          n = len(x_new)\n",
    "          yest_test = []\n",
    "          #Looping through all x-points\n",
    "          for i in range(n):\n",
    "            lm.fit(np.diag(w[:,i])@x,np.diag(w[:,i])@y)\n",
    "            yest_test.append(lm.predict([x_new[i]]))\n",
    "        return np.array(yest_test).flatten()\n",
    "        \n",
    "    def is_fitted(self):\n",
    "       return check_is_fitted(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "76d59cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boost:\n",
    "    def __init__(self, model1, model2):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.xtrain_ = x\n",
    "        self.ytrain_ = y\n",
    "    \n",
    "    def is_fitted(self):\n",
    "        return check_is_fitted(self)\n",
    "    \n",
    "    def predict(self, x_new, boost_iter):\n",
    "        model1 = self.model1\n",
    "        model2 = self.model2\n",
    "        model1.fit(self.xtrain_, self.ytrain_)\n",
    "        \n",
    "        yhat_train = model1.predict(self.xtrain_)\n",
    "        residuals_train = self.ytrain_ - yhat_train\n",
    "        model2.fit(self.xtrain_, residuals_train)\n",
    "        \n",
    "        for _ in range(boost_iter):\n",
    "            residuals_hat = model2.predict(x_new)\n",
    "            yhat_lw = model1.predict(x_new) + residuals_hat\n",
    "            model1.fit(x_new, yhat_lw)\n",
    "            yhat_train = model1.predict(self.xtrain_)\n",
    "            residuals_train = self.ytrain_ - yhat_train\n",
    "            model2.fit(self.xtrain_, residuals_train)\n",
    "        \n",
    "        yhat_lw = model1.predict(x_new) + model2.predict(x_new)\n",
    "        return yhat_lw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276722b2",
   "metadata": {},
   "source": [
    "### Testing Scalars On Real Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3c71f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = pd.read_csv('Data/nba.csv')\n",
    "x = nba.drop(['Date','Matchup','Spread','Margin'],axis=1).to_numpy()[:-2]\n",
    "y = nba['Margin'].to_numpy()[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c6e3606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cross-validated Mean Squared Error for QuantileTransformer : 171.29336283977153\n",
      "The Cross-validated Mean Squared Error for MinMaxScaler : 164.7207573152529\n",
      "The Cross-validated Mean Squared Error for StandardScaler : 359.83740436352895\n"
     ]
    }
   ],
   "source": [
    "mse_quant = []\n",
    "mse_mm = []\n",
    "mse_ss = []\n",
    "quant_scale = QuantileTransformer(n_quantiles=350)\n",
    "mm_scale = MinMaxScaler()\n",
    "ss_scale = StandardScaler()\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "model_1 = Lowess(kernel=Gaussian,tau=0.9)\n",
    "model_2 = Lowess(kernel=Tricubic,tau=0.9)\n",
    "full_model = Boost(model_1, model_2)\n",
    "\n",
    "for idxtrain, idxtest in kf.split(x):\n",
    "  xtrain = x[idxtrain]\n",
    "  ytrain = y[idxtrain].ravel()\n",
    "  ytest = y[idxtest].ravel()\n",
    "  xtest = x[idxtest]\n",
    "  xtrain_q = quant_scale.fit_transform(xtrain)\n",
    "  xtrain_mm = mm_scale.fit_transform(xtrain)\n",
    "  xtrain_ss = ss_scale.fit_transform(xtrain)\n",
    "\n",
    "  xtest_q = quant_scale.transform(xtest)\n",
    "  xtest_mm = mm_scale.transform(xtest)\n",
    "  xtest_ss = ss_scale.transform(xtest)\n",
    "\n",
    "  full_model.fit(xtrain_q,ytrain)\n",
    "  ypred = full_model.predict(xtest_q,3)\n",
    "  mse_quant.append(mse(ytest,ypred))\n",
    "\n",
    "  full_model.fit(xtrain_mm,ytrain)\n",
    "  ypred = full_model.predict(xtest_mm,3)\n",
    "  mse_mm.append(mse(ytest,ypred))\n",
    "\n",
    "  full_model.fit(xtrain_ss,ytrain)\n",
    "  ypred = full_model.predict(xtest_ss,3)\n",
    "  mse_ss.append(mse(ytest,ypred))\n",
    "\n",
    "print('The Cross-validated Mean Squared Error for QuantileTransformer : '+str(np.mean(mse_quant)))\n",
    "print('The Cross-validated Mean Squared Error for MinMaxScaler : '+str(np.mean(mse_mm)))\n",
    "print('The Cross-validated Mean Squared Error for StandardScaler : '+str(np.mean(mse_ss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650aab31",
   "metadata": {},
   "source": [
    "The MinMaxScaler has the best score followed by the QuantileTransformer and StandardScaler. The StandardScaler scores better with lower values of tau in model 1 and model 2. Overall, the MSE of 164 is much better than the MSE of 178 I was able to obtain in the last homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba41fd",
   "metadata": {},
   "source": [
    "### Application On Concrete Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ce28da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/concrete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "efec90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(columns=['strength']).values\n",
    "y = data['strength'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "73867296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cross-validated Mean Squared Error for Boosted Locally Weighted Regression is : 19.518017495330895\n",
      "The Cross-validated Mean Squared Error for a DT-based method: 20.18308623182933\n"
     ]
    }
   ],
   "source": [
    "mse_lwr = []\n",
    "mse_rf = []\n",
    "scale = QuantileTransformer(n_quantiles=900)\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "model_rf = XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=7)\n",
    "model_1 = Lowess(kernel=Gaussian,tau=0.325)\n",
    "model_2 = Lowess(kernel=Tricubic,tau=0.225)\n",
    "full_model = Boost(model_1, model_2)\n",
    "\n",
    "for idxtrain, idxtest in kf.split(x):\n",
    "  xtrain = x[idxtrain]\n",
    "  ytrain = y[idxtrain].ravel()\n",
    "  ytest = y[idxtest].ravel()\n",
    "  xtest = x[idxtest]\n",
    "  xtrain = scale.fit_transform(xtrain)\n",
    "  xtest = scale.transform(xtest)\n",
    "\n",
    "  full_model.fit(xtrain,ytrain)\n",
    "  ypred = full_model.predict(xtest,1)\n",
    "\n",
    "  model_rf.fit(xtrain,ytrain)\n",
    "  yhat_rf = model_rf.predict(xtest)\n",
    "\n",
    "  mse_lwr.append(mse(ytest,ypred))\n",
    "  mse_rf.append(mse(ytest,yhat_rf))\n",
    "print('The Cross-validated Mean Squared Error for Boosted Locally Weighted Regression is : '+str(np.mean(mse_lwr)))\n",
    "print('The Cross-validated Mean Squared Error for a DT-based method: '+str(np.mean(mse_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597480f7",
   "metadata": {},
   "source": [
    "With these hyperparameters, my implementation of gradient boosting beats XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9ef32-e874-49e2-8090-85956c897ad3",
   "metadata": {
    "id": "hwP4tTPhZISV"
   },
   "source": [
    "---\n",
    "### Task 2:\n",
    "Based on the Usearch library, create your own class that computes the k_Nearest Neighbors for Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6adae7c-3e1d-40e0-a7a8-42069068e47d",
   "metadata": {
    "id": "BwNDALB-ZIVf"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6840f172-0417-44d3-9129-bd608aef1ec9",
   "metadata": {
    "id": "6WEQCL13HnYC"
   },
   "outputs": [],
   "source": [
    "from usearch.index import search, MetricKind\n",
    "\n",
    "class KNN_Reg:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.xtrain_ = x\n",
    "        self.ytrain_ = y\n",
    "    \n",
    "    def is_fitted(self):\n",
    "        return check_is_fitted(self)\n",
    "    \n",
    "    def predict(self, x_new):\n",
    "        kneighbors = self.__get_knn(x_new)\n",
    "        if len(kneighbors.shape) > 1:\n",
    "            return [np.mean(row) for row in self.ytrain_[kneighbors]]\n",
    "        else:\n",
    "            return np.mean(self.ytrain_[kneighbors])\n",
    "    \n",
    "    def __get_knn(self, x_new):\n",
    "        neighbors = search(self.xtrain_, x_new, self.n_neighbors, MetricKind.L2sq, exact=True)\n",
    "        return neighbors.keys\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2a621",
   "metadata": {},
   "source": [
    "## Testing My Class on NBA Game Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "335d5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('Data/concrete.csv')\n",
    "# x = data.drop(columns=['strength']).values\n",
    "# y = data['strength'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5c2d7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = pd.read_csv('Data/nba.csv')\n",
    "x = nba.drop(['Date','Matchup','Spread','Margin'],axis=1).to_numpy()[:-2]\n",
    "y = nba['Margin'].to_numpy()[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fb233cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cross-validated Mean Squared Error for KNN Regression is : 168.82562564478098\n"
     ]
    }
   ],
   "source": [
    "mse_knn = []\n",
    "scale = QuantileTransformer(n_quantiles=500)\n",
    "\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=1234)\n",
    "model_knn = KNN_Reg(55)\n",
    "\n",
    "for idxtrain, idxtest in kf.split(x):\n",
    "  xtrain = x[idxtrain]\n",
    "  ytrain = y[idxtrain].ravel()\n",
    "  ytest = y[idxtest].ravel()\n",
    "  xtest = x[idxtest]\n",
    "  xtrain = scale.fit_transform(xtrain)\n",
    "  xtest = scale.transform(xtest)\n",
    "\n",
    "  model_knn.fit(xtrain,ytrain)\n",
    "  ypred = model_knn.predict(xtest)\n",
    "\n",
    "  mse_knn.append(mse(ytest,ypred))\n",
    "print('The Cross-validated Mean Squared Error for KNN Regression is : '+str(np.mean(mse_knn)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a533a0",
   "metadata": {},
   "source": [
    "This is almost as good as the mse obtained from the gradient boosted Lowess model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ee8bc-2290-4eb5-9544-df9ea95bf7ee",
   "metadata": {
    "id": "u0P3-vfRjAqu"
   },
   "source": [
    "---\n",
    "### Task 3\n",
    "Host your project on your GitHub page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ea5aa",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "https://github.com/Pschnizer/DATA441/blob/main/DATA_441_Project_2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
