{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peter Schnizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Task 1:\n",
    "Create your own PyTorch class that implements the method of SCAD regularization and variable selection (smoothly clipped absolute deviations) for linear models. Your development should be based on the following references:\n",
    "\n",
    "https://andrewcharlesjones.github.io/journal/scad.html\n",
    "\n",
    "https://www.jstor.org/stable/27640214?seq=1\n",
    "\n",
    "Test your method one a real data set, and determine a variable selection based on features importance according to SCAD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # we are going to use pytorch instead of numpy because it's much faster.\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import toeplitz\n",
    "from sklearn.metrics import mean_absolute_error as mae, mean_squared_error as mse, r2_score as R2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNet(nn.Module):\n",
    "    def __init__(self, input_size, alpha=1.0, l1_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the ElasticNet regression model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            alpha (float): Regularization strength. Higher values of alpha\n",
    "                emphasize L1 regularization, while lower values emphasize L2 regularization.\n",
    "            l1_ratio (float): The ratio of L1 regularization to the total\n",
    "                regularization (L1 + L2). It should be between 0 and 1.\n",
    "\n",
    "        \"\"\"\n",
    "        super(ElasticNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "        # Define the linear regression layer\n",
    "        self.linear = nn.Linear(input_size, 1,bias=False,device=device,dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ElasticNet model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input data with shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (batch_size, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the ElasticNet loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
    "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The ElasticNet loss.\n",
    "\n",
    "        \"\"\"\n",
    "        mse_loss = nn.MSELoss(reduction='mean')(y_pred, y_true)\n",
    "        l1_reg = torch.norm(self.linear.weight, p=1)\n",
    "        l2_reg = torch.norm(self.linear.weight, p=2)\n",
    "\n",
    "        objective = (1/2) * mse_loss + self.alpha * (\n",
    "            self.l1_ratio * l1_reg + (1 - self.l1_ratio) * (1/2)*l2_reg**2)\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def fit(self, X, y, num_epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the ElasticNet model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "            y (Tensor): Target values with shape (num_samples, 1).\n",
    "            num_epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimization.\n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X) # a forward propagation\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss.backward() # the optimizer computes the gradient\n",
    "            optimizer.step() # the optimizer updates the weights according to the chosen heuristics\n",
    "\n",
    "            if (epoch + 1) % 10 == 0: # decide to display the progress\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (num_samples, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(X)\n",
    "        return y_pred\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Get the coefficients (weights) of the linear regression layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Coefficients with shape (output_size, input_size).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCAD(nn.Module):\n",
    "    def __init__(self, input_size, alpha=0.5, lmda=0.5):\n",
    "        super(SCAD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.alpha = alpha\n",
    "        self.lmda = lmda\n",
    "\n",
    "        # Define the linear regression layer\n",
    "        self.linear = nn.Linear(input_size, 1,bias=False,device=device,dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the SCAD model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input data with shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (batch_size, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def scad_penalty(self):\n",
    "        is_linear = (torch.abs(self.linear.weight) <= self.lmda)\n",
    "        is_quadratic = torch.logical_and(self.lmda < torch.abs(self.linear.weight), torch.abs(self.linear.weight) <= self.alpha * self.lmda)\n",
    "        is_constant = (self.alpha * self.lmda) < torch.abs(self.linear.weight)\n",
    "\n",
    "        linear_part = self.lmda * torch.abs(self.linear.weight) * is_linear\n",
    "        quadratic_part = (2 * self.alpha * self.lmda * torch.abs(self.linear.weight) - self.linear.weight**2 - self.lmda**2) / (2 * (self.alpha - 1)) * is_quadratic\n",
    "        constant_part = (self.lmda**2 * (self.alpha + 1)) / 2 * is_constant\n",
    "        return linear_part + quadratic_part + constant_part\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the SCAD loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
    "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The SCAD loss.\n",
    "\n",
    "        \"\"\"\n",
    "        mse_loss = nn.MSELoss(reduction='mean')(y_pred, y_true)\n",
    "        scad_reg = torch.norm(self.scad_penalty(), p=1)\n",
    "        objective = mse_loss + scad_reg\n",
    "        return objective\n",
    "\n",
    "    def fit(self, X, y, num_epochs=100, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the SCAD model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "            y (Tensor): Target values with shape (num_samples, 1).\n",
    "            num_epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimization.\n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X) # a forward propagation\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss.backward() # the optimizer computes the gradient\n",
    "            optimizer.step() # the optimizer updates the weights according to the chosen heuristics\n",
    "\n",
    "            if (epoch + 1) % 10 == 0: # decide to display the progress\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (num_samples, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(X)\n",
    "        return y_pred\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Get the coefficients (weights) of the linear regression layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Coefficients with shape (output_size, input_size).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PPG', 'Rebs', 'Stls', 'Blks', 'TOVs', 'FG%', '3FG%', 'Off Rtg',\n",
       "       'Def Rtg', 'Pace', 'Lost Prod'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on nba dataset\n",
    "nba = pd.read_csv('Data/nba.csv')\n",
    "X = nba.drop(['Date','Matchup','Spread','Margin'],axis=1).to_numpy()[:-2]\n",
    "y = nba['Margin'].to_numpy()[:-2]\n",
    "X_cols = nba.drop(['Date','Matchup','Spread','Margin'],axis=1).columns\n",
    "X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 195.93380737304688\n",
      "Epoch [20/100], Loss: 194.15472412109375\n",
      "Epoch [30/100], Loss: 193.335205078125\n",
      "Epoch [40/100], Loss: 192.94654846191406\n",
      "Epoch [50/100], Loss: 192.76776123046875\n",
      "Epoch [60/100], Loss: 192.69390869140625\n",
      "Epoch [70/100], Loss: 192.6868438720703\n",
      "Epoch [80/100], Loss: 192.68145751953125\n",
      "Epoch [90/100], Loss: 192.6739044189453\n",
      "Epoch [100/100], Loss: 192.66575622558594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptsch\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([627])) that is different to the input size (torch.Size([627, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "scad_model = SCAD(input_size=X.shape[1], alpha=0.1, lmda=0.2)\n",
    "scad_model.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "enet_model = ElasticNet(input_size=X.shape[1], alpha=0.1, l1_ratio=0.1)\n",
    "\n",
    "scad_pred = scad_model.predict(X_test_tensor)\n",
    "enet_pred = enet_model.predict(X_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCAD MSE: 198.3649080353595\n",
      "ElasticNet MSE: 202.7778594511267\n"
     ]
    }
   ],
   "source": [
    "print(f'SCAD MSE: {mse(y_test, scad_pred)}\\nElasticNet MSE: {mse(y_test, enet_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SCAD regularization term works better than ElasticNet for the NBA data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Rankings:\n",
      "1: Off Rtg 0.5143522620201111\n",
      "2: Stls 0.513846755027771\n",
      "3: PPG 0.48982667922973633\n",
      "4: FG% 0.472359299659729\n",
      "5: 3FG% 0.43158531188964844\n",
      "6: Pace 0.3675922751426697\n",
      "7: Blks 0.2889552116394043\n",
      "8: Def Rtg 0.2420038878917694\n",
      "9: Rebs 0.15583162009716034\n",
      "10: Lost Prod 0.13700784742832184\n",
      "11: TOVs 0.13058532774448395\n"
     ]
    }
   ],
   "source": [
    "coeffs = scad_model.get_coefficients()[0]\n",
    "ranked_coeffs_idxs = torch.abs(coeffs).argsort(descending=True)\n",
    "ranked_ftrs = np.array(X_cols)[ranked_coeffs_idxs]\n",
    "print('Feature Rankings:')\n",
    "for i, (idx, feature) in enumerate(zip(ranked_coeffs_idxs, ranked_ftrs)):\n",
    "    print(str(i+1)+':', feature,  coeffs.tolist()[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature selection I would try based off of these results would be minimizing the model to just Off Rtg, Stls, PPG, FG%, and 3FG%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 2:\n",
    "Based on the simulation design explained in class, generate 500 data sets where the input features have a strong correlation structure (you may consider a 0.9) and apply ElasticNet, SqrtLasso and SCAD to check which method produces the best approximation of an ideal solution, such as a \"betastar\" you design with a sparsity pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqrtLasso(nn.Module):\n",
    "    def __init__(self, input_size, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the  regression model.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        super(SqrtLasso, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "        # Define the linear regression layer\n",
    "        self.linear = nn.Linear(input_size, 1,bias=False,device=device,dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input data with shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (batch_size, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (Tensor): Predicted values with shape (batch_size, 1).\n",
    "            y_true (Tensor): True target values with shape (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The loss.\n",
    "\n",
    "        \"\"\"\n",
    "        mse_loss = nn.MSELoss(reduction='mean')(y_pred, y_true)\n",
    "        l1_reg = torch.norm(self.linear.weight, p=1,dtype=torch.float64)\n",
    "        # l2_reg = torch.norm(self.linear.weight, p=2,dtype=torch.float64)\n",
    "\n",
    "        loss = torch.sqrt(mse_loss) + self.alpha * (l1_reg)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y, num_epochs=500, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "            y (Tensor): Target values with shape (num_samples, 1).\n",
    "            num_epochs (int): Number of training epochs.\n",
    "            learning_rate (float): Learning rate for optimization.\n",
    "\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = self(X)\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Input data with shape (num_samples, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted values with shape (num_samples, 1).\n",
    "\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(X)\n",
    "        return y_pred\n",
    "    def get_coefficients(self):\n",
    "        \"\"\"\n",
    "        Get the coefficients (weights) of the linear regression layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Coefficients with shape (output_size, input_size).\n",
    "\n",
    "        \"\"\"\n",
    "        return self.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_correlated_features(num_samples,p,rho):\n",
    "  vcor = [] \n",
    "  for i in range(p):\n",
    "    vcor.append(rho**i)\n",
    "  r = toeplitz(vcor)\n",
    "  mu = np.repeat(0,p)\n",
    "  x = np.random.multivariate_normal(mu, r, size=num_samples)\n",
    "  return x\n",
    "\n",
    "p = 200\n",
    "n = 500\n",
    "rho = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 200)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = make_correlated_features(n,p,rho)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta =np.array([0, -1,2,3,0,0,0,0,2,-1,4])\n",
    "beta = beta.reshape(-1,1)\n",
    "betastar = np.concatenate([beta,np.repeat(0,p-len(beta)).reshape(-1,1)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x@betastar + 1.5*np.random.normal(size=(n,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.tensor(x,device=device, dtype=torch.float32)\n",
    "y_torch = torch.tensor(y,device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 20.341869354248047\n",
      "Epoch [20/10000], Loss: 14.503839492797852\n",
      "Epoch [30/10000], Loss: 10.744985580444336\n",
      "Epoch [40/10000], Loss: 8.270544052124023\n",
      "Epoch [50/10000], Loss: 6.648612976074219\n",
      "Epoch [60/10000], Loss: 5.555250644683838\n",
      "Epoch [70/10000], Loss: 4.793609619140625\n",
      "Epoch [80/10000], Loss: 4.264492988586426\n",
      "Epoch [90/10000], Loss: 3.8985838890075684\n",
      "Epoch [100/10000], Loss: 3.6425962448120117\n",
      "Epoch [110/10000], Loss: 3.4643428325653076\n",
      "Epoch [120/10000], Loss: 3.3425872325897217\n",
      "Epoch [130/10000], Loss: 3.253056764602661\n",
      "Epoch [140/10000], Loss: 3.1857829093933105\n",
      "Epoch [150/10000], Loss: 3.133524179458618\n",
      "Epoch [160/10000], Loss: 3.0889980792999268\n",
      "Epoch [170/10000], Loss: 3.0499088764190674\n",
      "Epoch [180/10000], Loss: 3.0148208141326904\n",
      "Epoch [190/10000], Loss: 2.9829249382019043\n",
      "Epoch [200/10000], Loss: 2.951894760131836\n",
      "Epoch [210/10000], Loss: 2.922621488571167\n",
      "Epoch [220/10000], Loss: 2.89424729347229\n",
      "Epoch [230/10000], Loss: 2.8668668270111084\n",
      "Epoch [240/10000], Loss: 2.8405110836029053\n",
      "Epoch [250/10000], Loss: 2.8157289028167725\n",
      "Epoch [260/10000], Loss: 2.791800022125244\n",
      "Epoch [270/10000], Loss: 2.7681212425231934\n",
      "Epoch [280/10000], Loss: 2.7459418773651123\n",
      "Epoch [290/10000], Loss: 2.7240419387817383\n",
      "Epoch [300/10000], Loss: 2.7039718627929688\n",
      "Epoch [310/10000], Loss: 2.6832919120788574\n",
      "Epoch [320/10000], Loss: 2.662526845932007\n",
      "Epoch [330/10000], Loss: 2.642709255218506\n",
      "Epoch [340/10000], Loss: 2.6242079734802246\n",
      "Epoch [350/10000], Loss: 2.6058130264282227\n",
      "Epoch [360/10000], Loss: 2.5874686241149902\n",
      "Epoch [370/10000], Loss: 2.569272041320801\n",
      "Epoch [380/10000], Loss: 2.5521767139434814\n",
      "Epoch [390/10000], Loss: 2.5358028411865234\n",
      "Epoch [400/10000], Loss: 2.520378351211548\n",
      "Epoch [410/10000], Loss: 2.5045604705810547\n",
      "Epoch [420/10000], Loss: 2.489478826522827\n",
      "Epoch [430/10000], Loss: 2.4745090007781982\n",
      "Epoch [440/10000], Loss: 2.460963726043701\n",
      "Epoch [450/10000], Loss: 2.446782350540161\n",
      "Epoch [460/10000], Loss: 2.432868003845215\n",
      "Epoch [470/10000], Loss: 2.4198904037475586\n",
      "Epoch [480/10000], Loss: 2.4070310592651367\n",
      "Epoch [490/10000], Loss: 2.3947160243988037\n",
      "Epoch [500/10000], Loss: 2.3827290534973145\n",
      "Epoch [510/10000], Loss: 2.3717851638793945\n",
      "Epoch [520/10000], Loss: 2.360931873321533\n",
      "Epoch [530/10000], Loss: 2.3499763011932373\n",
      "Epoch [540/10000], Loss: 2.3390326499938965\n",
      "Epoch [550/10000], Loss: 2.3276660442352295\n",
      "Epoch [560/10000], Loss: 2.31911563873291\n",
      "Epoch [570/10000], Loss: 2.309568405151367\n",
      "Epoch [580/10000], Loss: 2.3006560802459717\n",
      "Epoch [590/10000], Loss: 2.2914328575134277\n",
      "Epoch [600/10000], Loss: 2.283971071243286\n",
      "Epoch [610/10000], Loss: 2.275207281112671\n",
      "Epoch [620/10000], Loss: 2.266876220703125\n",
      "Epoch [630/10000], Loss: 2.258448600769043\n",
      "Epoch [640/10000], Loss: 2.2520127296447754\n",
      "Epoch [650/10000], Loss: 2.245189666748047\n",
      "Epoch [660/10000], Loss: 2.2395129203796387\n",
      "Epoch [670/10000], Loss: 2.233085870742798\n",
      "Epoch [680/10000], Loss: 2.2270219326019287\n",
      "Epoch [690/10000], Loss: 2.220278739929199\n",
      "Epoch [700/10000], Loss: 2.2143919467926025\n",
      "Epoch [710/10000], Loss: 2.2092533111572266\n",
      "Epoch [720/10000], Loss: 2.2038776874542236\n",
      "Epoch [730/10000], Loss: 2.1986188888549805\n",
      "Epoch [740/10000], Loss: 2.193819046020508\n",
      "Epoch [750/10000], Loss: 2.188865900039673\n",
      "Epoch [760/10000], Loss: 2.184468984603882\n",
      "Epoch [770/10000], Loss: 2.1812214851379395\n",
      "Epoch [780/10000], Loss: 2.17665958404541\n",
      "Epoch [790/10000], Loss: 2.172880172729492\n",
      "Epoch [800/10000], Loss: 2.1687710285186768\n",
      "Epoch [810/10000], Loss: 2.165666103363037\n",
      "Epoch [820/10000], Loss: 2.161452531814575\n",
      "Epoch [830/10000], Loss: 2.1581835746765137\n",
      "Epoch [840/10000], Loss: 2.1545722484588623\n",
      "Epoch [850/10000], Loss: 2.150275707244873\n",
      "Epoch [860/10000], Loss: 2.1484110355377197\n",
      "Epoch [870/10000], Loss: 2.1450772285461426\n",
      "Epoch [880/10000], Loss: 2.142503023147583\n",
      "Epoch [890/10000], Loss: 2.138634204864502\n",
      "Epoch [900/10000], Loss: 2.1373512744903564\n",
      "Epoch [910/10000], Loss: 2.13454008102417\n",
      "Epoch [920/10000], Loss: 2.1314806938171387\n",
      "Epoch [930/10000], Loss: 2.1285462379455566\n",
      "Epoch [940/10000], Loss: 2.1261255741119385\n",
      "Epoch [950/10000], Loss: 2.1244869232177734\n",
      "Epoch [960/10000], Loss: 2.122464418411255\n",
      "Epoch [970/10000], Loss: 2.1210079193115234\n",
      "Epoch [980/10000], Loss: 2.1199026107788086\n",
      "Epoch [990/10000], Loss: 2.1185643672943115\n",
      "Epoch [1000/10000], Loss: 2.1168601512908936\n",
      "Epoch [1010/10000], Loss: 2.114861011505127\n",
      "Epoch [1020/10000], Loss: 2.113110303878784\n",
      "Epoch [1030/10000], Loss: 2.1111929416656494\n",
      "Epoch [1040/10000], Loss: 2.110180377960205\n",
      "Epoch [1050/10000], Loss: 2.1096081733703613\n",
      "Epoch [1060/10000], Loss: 2.109137535095215\n",
      "Epoch [1070/10000], Loss: 2.106403350830078\n",
      "Epoch [1080/10000], Loss: 2.1069767475128174\n",
      "Epoch [1090/10000], Loss: 2.105618953704834\n",
      "Epoch [1100/10000], Loss: 2.103121280670166\n",
      "Epoch [1110/10000], Loss: 2.1029558181762695\n",
      "Epoch [1120/10000], Loss: 2.1022167205810547\n",
      "Epoch [1130/10000], Loss: 2.1010868549346924\n",
      "Epoch [1140/10000], Loss: 2.0997726917266846\n",
      "Epoch [1150/10000], Loss: 2.0999879837036133\n",
      "Epoch [1160/10000], Loss: 2.0984926223754883\n",
      "Epoch [1170/10000], Loss: 2.0971274375915527\n",
      "Epoch [1180/10000], Loss: 2.096634864807129\n",
      "Epoch [1190/10000], Loss: 2.0959854125976562\n",
      "Epoch [1200/10000], Loss: 2.0948805809020996\n",
      "Epoch [1210/10000], Loss: 2.094780206680298\n",
      "Epoch [1220/10000], Loss: 2.094863176345825\n",
      "Epoch [1230/10000], Loss: 2.0934789180755615\n",
      "Epoch [1240/10000], Loss: 2.091736078262329\n",
      "Epoch [1250/10000], Loss: 2.0922698974609375\n",
      "Epoch [1260/10000], Loss: 2.0923564434051514\n",
      "Epoch [1270/10000], Loss: 2.0908756256103516\n",
      "Epoch [1280/10000], Loss: 2.0900650024414062\n",
      "Epoch [1290/10000], Loss: 2.089773416519165\n",
      "Epoch [1300/10000], Loss: 2.0888004302978516\n",
      "Epoch [1310/10000], Loss: 2.0892159938812256\n",
      "Epoch [1320/10000], Loss: 2.0889413356781006\n",
      "Epoch [1330/10000], Loss: 2.0884931087493896\n",
      "Epoch [1340/10000], Loss: 2.0882177352905273\n",
      "Epoch [1350/10000], Loss: 2.087648868560791\n",
      "Epoch [1360/10000], Loss: 2.0876758098602295\n",
      "Epoch [1370/10000], Loss: 2.08625864982605\n",
      "Epoch [1380/10000], Loss: 2.087714910507202\n",
      "Epoch [1390/10000], Loss: 2.085505247116089\n",
      "Epoch [1400/10000], Loss: 2.084388256072998\n",
      "Epoch [1410/10000], Loss: 2.0841293334960938\n",
      "Epoch [1420/10000], Loss: 2.0851428508758545\n",
      "Epoch [1430/10000], Loss: 2.0854909420013428\n",
      "Epoch [1440/10000], Loss: 2.083686351776123\n",
      "Epoch [1450/10000], Loss: 2.0824670791625977\n",
      "Epoch [1460/10000], Loss: 2.0832056999206543\n",
      "Epoch [1470/10000], Loss: 2.083312749862671\n",
      "Epoch [1480/10000], Loss: 2.0838379859924316\n",
      "Epoch [1490/10000], Loss: 2.083110809326172\n",
      "Epoch [1500/10000], Loss: 2.0835769176483154\n",
      "Epoch [1510/10000], Loss: 2.082456111907959\n",
      "Epoch [1520/10000], Loss: 2.0824077129364014\n",
      "Epoch [1530/10000], Loss: 2.082009792327881\n",
      "Epoch [1540/10000], Loss: 2.080977439880371\n",
      "Epoch [1550/10000], Loss: 2.0819056034088135\n",
      "Epoch [1560/10000], Loss: 2.0827178955078125\n",
      "Epoch [1570/10000], Loss: 2.0826025009155273\n",
      "Epoch [1580/10000], Loss: 2.082350254058838\n",
      "Epoch [1590/10000], Loss: 2.0817711353302\n",
      "Epoch [1600/10000], Loss: 2.0807785987854004\n",
      "Epoch [1610/10000], Loss: 2.080991506576538\n",
      "Epoch [1620/10000], Loss: 2.0816547870635986\n",
      "Epoch [1630/10000], Loss: 2.080524444580078\n",
      "Epoch [1640/10000], Loss: 2.080615997314453\n",
      "Epoch [1650/10000], Loss: 2.0813393592834473\n",
      "Epoch [1660/10000], Loss: 2.0800933837890625\n",
      "Epoch [1670/10000], Loss: 2.0798161029815674\n",
      "Epoch [1680/10000], Loss: 2.080726146697998\n",
      "Epoch [1690/10000], Loss: 2.080458879470825\n",
      "Epoch [1700/10000], Loss: 2.0802431106567383\n",
      "Epoch [1710/10000], Loss: 2.0801563262939453\n",
      "Epoch [1720/10000], Loss: 2.079753875732422\n",
      "Epoch [1730/10000], Loss: 2.078977108001709\n",
      "Epoch [1740/10000], Loss: 2.0799758434295654\n",
      "Epoch [1750/10000], Loss: 2.0801899433135986\n",
      "Epoch [1760/10000], Loss: 2.0797832012176514\n",
      "Epoch [1770/10000], Loss: 2.0798826217651367\n",
      "Epoch [1780/10000], Loss: 2.0783205032348633\n",
      "Epoch [1790/10000], Loss: 2.0788309574127197\n",
      "Epoch [1800/10000], Loss: 2.0793917179107666\n",
      "Epoch [1810/10000], Loss: 2.0797371864318848\n",
      "Epoch [1820/10000], Loss: 2.079036235809326\n",
      "Epoch [1830/10000], Loss: 2.0789830684661865\n",
      "Epoch [1840/10000], Loss: 2.079195976257324\n",
      "Epoch [1850/10000], Loss: 2.0792641639709473\n",
      "Epoch [1860/10000], Loss: 2.0792293548583984\n",
      "Epoch [1870/10000], Loss: 2.0792434215545654\n",
      "Epoch [1880/10000], Loss: 2.0788509845733643\n",
      "Epoch [1890/10000], Loss: 2.079681396484375\n",
      "Epoch [1900/10000], Loss: 2.0791099071502686\n",
      "Epoch [1910/10000], Loss: 2.079007148742676\n",
      "Epoch [1920/10000], Loss: 2.078934669494629\n",
      "Epoch [1930/10000], Loss: 2.0777640342712402\n",
      "Epoch [1940/10000], Loss: 2.078798770904541\n",
      "Epoch [1950/10000], Loss: 2.079864501953125\n",
      "Epoch [1960/10000], Loss: 2.079045534133911\n",
      "Epoch [1970/10000], Loss: 2.078885793685913\n",
      "Epoch [1980/10000], Loss: 2.0792617797851562\n",
      "Epoch [1990/10000], Loss: 2.0793116092681885\n",
      "Epoch [2000/10000], Loss: 2.0792245864868164\n",
      "Epoch [2010/10000], Loss: 2.0786757469177246\n",
      "Epoch [2020/10000], Loss: 2.0803093910217285\n",
      "Epoch [2030/10000], Loss: 2.0786845684051514\n",
      "Epoch [2040/10000], Loss: 2.0800368785858154\n",
      "Epoch [2050/10000], Loss: 2.077897787094116\n",
      "Epoch [2060/10000], Loss: 2.0790979862213135\n",
      "Epoch [2070/10000], Loss: 2.078437089920044\n",
      "Epoch [2080/10000], Loss: 2.078397274017334\n",
      "Epoch [2090/10000], Loss: 2.0798048973083496\n",
      "Epoch [2100/10000], Loss: 2.0793659687042236\n",
      "Epoch [2110/10000], Loss: 2.0783514976501465\n",
      "Epoch [2120/10000], Loss: 2.07905912399292\n",
      "Epoch [2130/10000], Loss: 2.0783236026763916\n",
      "Epoch [2140/10000], Loss: 2.0795416831970215\n",
      "Epoch [2150/10000], Loss: 2.0787134170532227\n",
      "Epoch [2160/10000], Loss: 2.078805923461914\n",
      "Epoch [2170/10000], Loss: 2.0785515308380127\n",
      "Epoch [2180/10000], Loss: 2.0785229206085205\n",
      "Epoch [2190/10000], Loss: 2.079029083251953\n",
      "Epoch [2200/10000], Loss: 2.078796625137329\n",
      "Epoch [2210/10000], Loss: 2.0800161361694336\n",
      "Epoch [2220/10000], Loss: 2.0783722400665283\n",
      "Epoch [2230/10000], Loss: 2.078674793243408\n",
      "Epoch [2240/10000], Loss: 2.0800280570983887\n",
      "Epoch [2250/10000], Loss: 2.078350782394409\n",
      "Epoch [2260/10000], Loss: 2.0795841217041016\n",
      "Epoch [2270/10000], Loss: 2.0785670280456543\n",
      "Epoch [2280/10000], Loss: 2.078547716140747\n",
      "Epoch [2290/10000], Loss: 2.0800294876098633\n",
      "Epoch [2300/10000], Loss: 2.0787253379821777\n",
      "Epoch [2310/10000], Loss: 2.078655958175659\n",
      "Epoch [2320/10000], Loss: 2.079115867614746\n",
      "Epoch [2330/10000], Loss: 2.0789029598236084\n",
      "Epoch [2340/10000], Loss: 2.0788981914520264\n",
      "Epoch [2350/10000], Loss: 2.078350305557251\n",
      "Epoch [2360/10000], Loss: 2.078082799911499\n",
      "Epoch [2370/10000], Loss: 2.0781166553497314\n",
      "Epoch [2380/10000], Loss: 2.079763889312744\n",
      "Epoch [2390/10000], Loss: 2.0798916816711426\n",
      "Epoch [2400/10000], Loss: 2.078584671020508\n",
      "Epoch [2410/10000], Loss: 2.0792877674102783\n",
      "Epoch [2420/10000], Loss: 2.079754590988159\n",
      "Epoch [2430/10000], Loss: 2.0798494815826416\n",
      "Epoch [2440/10000], Loss: 2.079293966293335\n",
      "Epoch [2450/10000], Loss: 2.0781354904174805\n",
      "Epoch [2460/10000], Loss: 2.0792298316955566\n",
      "Epoch [2470/10000], Loss: 2.078925848007202\n",
      "Epoch [2480/10000], Loss: 2.0780744552612305\n",
      "Epoch [2490/10000], Loss: 2.0787692070007324\n",
      "Epoch [2500/10000], Loss: 2.0779058933258057\n",
      "Epoch [2510/10000], Loss: 2.078493356704712\n",
      "Epoch [2520/10000], Loss: 2.078270196914673\n",
      "Epoch [2530/10000], Loss: 2.0781805515289307\n",
      "Epoch [2540/10000], Loss: 2.078247308731079\n",
      "Epoch [2550/10000], Loss: 2.0780351161956787\n",
      "Epoch [2560/10000], Loss: 2.079368829727173\n",
      "Epoch [2570/10000], Loss: 2.0793142318725586\n",
      "Epoch [2580/10000], Loss: 2.078774929046631\n",
      "Epoch [2590/10000], Loss: 2.078385829925537\n",
      "Epoch [2600/10000], Loss: 2.0784664154052734\n",
      "Epoch [2610/10000], Loss: 2.078274965286255\n",
      "Epoch [2620/10000], Loss: 2.0785303115844727\n",
      "Epoch [2630/10000], Loss: 2.0782158374786377\n",
      "Epoch [2640/10000], Loss: 2.078619956970215\n",
      "Epoch [2650/10000], Loss: 2.0784354209899902\n",
      "Epoch [2660/10000], Loss: 2.078374147415161\n",
      "Epoch [2670/10000], Loss: 2.0784523487091064\n",
      "Epoch [2680/10000], Loss: 2.0788707733154297\n",
      "Epoch [2690/10000], Loss: 2.078007459640503\n",
      "Epoch [2700/10000], Loss: 2.078397750854492\n",
      "Epoch [2710/10000], Loss: 2.0783042907714844\n",
      "Epoch [2720/10000], Loss: 2.0789358615875244\n",
      "Epoch [2730/10000], Loss: 2.07882022857666\n",
      "Epoch [2740/10000], Loss: 2.0782036781311035\n",
      "Epoch [2750/10000], Loss: 2.078432559967041\n",
      "Epoch [2760/10000], Loss: 2.0793893337249756\n",
      "Epoch [2770/10000], Loss: 2.0783801078796387\n",
      "Epoch [2780/10000], Loss: 2.0777244567871094\n",
      "Epoch [2790/10000], Loss: 2.079132318496704\n",
      "Epoch [2800/10000], Loss: 2.079042673110962\n",
      "Epoch [2810/10000], Loss: 2.0787811279296875\n",
      "Epoch [2820/10000], Loss: 2.078439235687256\n",
      "Epoch [2830/10000], Loss: 2.077723503112793\n",
      "Epoch [2840/10000], Loss: 2.078298568725586\n",
      "Epoch [2850/10000], Loss: 2.0787394046783447\n",
      "Epoch [2860/10000], Loss: 2.078423023223877\n",
      "Epoch [2870/10000], Loss: 2.078378438949585\n",
      "Epoch [2880/10000], Loss: 2.0784542560577393\n",
      "Epoch [2890/10000], Loss: 2.079115152359009\n",
      "Epoch [2900/10000], Loss: 2.079277515411377\n",
      "Epoch [2910/10000], Loss: 2.078854560852051\n",
      "Epoch [2920/10000], Loss: 2.077958583831787\n",
      "Epoch [2930/10000], Loss: 2.0790345668792725\n",
      "Epoch [2940/10000], Loss: 2.079679250717163\n",
      "Epoch [2950/10000], Loss: 2.0786795616149902\n",
      "Epoch [2960/10000], Loss: 2.0796704292297363\n",
      "Epoch [2970/10000], Loss: 2.0788776874542236\n",
      "Epoch [2980/10000], Loss: 2.079723834991455\n",
      "Epoch [2990/10000], Loss: 2.077970266342163\n",
      "Epoch [3000/10000], Loss: 2.0786986351013184\n",
      "Epoch [3010/10000], Loss: 2.078237533569336\n",
      "Epoch [3020/10000], Loss: 2.0782878398895264\n",
      "Epoch [3030/10000], Loss: 2.0786662101745605\n",
      "Epoch [3040/10000], Loss: 2.078913450241089\n",
      "Epoch [3050/10000], Loss: 2.078895092010498\n",
      "Epoch [3060/10000], Loss: 2.079423189163208\n",
      "Epoch [3070/10000], Loss: 2.0794410705566406\n",
      "Epoch [3080/10000], Loss: 2.0790014266967773\n",
      "Epoch [3090/10000], Loss: 2.078129291534424\n",
      "Epoch [3100/10000], Loss: 2.0798285007476807\n",
      "Epoch [3110/10000], Loss: 2.078080654144287\n",
      "Epoch [3120/10000], Loss: 2.078033685684204\n",
      "Epoch [3130/10000], Loss: 2.0797080993652344\n",
      "Epoch [3140/10000], Loss: 2.0788183212280273\n",
      "Epoch [3150/10000], Loss: 2.0794565677642822\n",
      "Epoch [3160/10000], Loss: 2.0779879093170166\n",
      "Epoch [3170/10000], Loss: 2.0793888568878174\n",
      "Epoch [3180/10000], Loss: 2.078248977661133\n",
      "Epoch [3190/10000], Loss: 2.0786869525909424\n",
      "Epoch [3200/10000], Loss: 2.0781776905059814\n",
      "Epoch [3210/10000], Loss: 2.0784506797790527\n",
      "Epoch [3220/10000], Loss: 2.078240394592285\n",
      "Epoch [3230/10000], Loss: 2.078261137008667\n",
      "Epoch [3240/10000], Loss: 2.078127384185791\n",
      "Epoch [3250/10000], Loss: 2.078474521636963\n",
      "Epoch [3260/10000], Loss: 2.0793333053588867\n",
      "Epoch [3270/10000], Loss: 2.0785398483276367\n",
      "Epoch [3280/10000], Loss: 2.078923225402832\n",
      "Epoch [3290/10000], Loss: 2.0790181159973145\n",
      "Epoch [3300/10000], Loss: 2.0791988372802734\n",
      "Epoch [3310/10000], Loss: 2.0780537128448486\n",
      "Epoch [3320/10000], Loss: 2.0791749954223633\n",
      "Epoch [3330/10000], Loss: 2.0791842937469482\n",
      "Epoch [3340/10000], Loss: 2.0778937339782715\n",
      "Epoch [3350/10000], Loss: 2.078826427459717\n",
      "Epoch [3360/10000], Loss: 2.0791678428649902\n",
      "Epoch [3370/10000], Loss: 2.079868793487549\n",
      "Epoch [3380/10000], Loss: 2.0787253379821777\n",
      "Epoch [3390/10000], Loss: 2.07948899269104\n",
      "Epoch [3400/10000], Loss: 2.079458475112915\n",
      "Epoch [3410/10000], Loss: 2.078646183013916\n",
      "Epoch [3420/10000], Loss: 2.0786163806915283\n",
      "Epoch [3430/10000], Loss: 2.0785279273986816\n",
      "Epoch [3440/10000], Loss: 2.0788326263427734\n",
      "Epoch [3450/10000], Loss: 2.0790834426879883\n",
      "Epoch [3460/10000], Loss: 2.0791983604431152\n",
      "Epoch [3470/10000], Loss: 2.0784831047058105\n",
      "Epoch [3480/10000], Loss: 2.078990936279297\n",
      "Epoch [3490/10000], Loss: 2.0787763595581055\n",
      "Epoch [3500/10000], Loss: 2.078918218612671\n",
      "Epoch [3510/10000], Loss: 2.079089879989624\n",
      "Epoch [3520/10000], Loss: 2.078916311264038\n",
      "Epoch [3530/10000], Loss: 2.079577922821045\n",
      "Epoch [3540/10000], Loss: 2.0791168212890625\n",
      "Epoch [3550/10000], Loss: 2.0787272453308105\n",
      "Epoch [3560/10000], Loss: 2.079129219055176\n",
      "Epoch [3570/10000], Loss: 2.0793206691741943\n",
      "Epoch [3580/10000], Loss: 2.07798433303833\n",
      "Epoch [3590/10000], Loss: 2.0796866416931152\n",
      "Epoch [3600/10000], Loss: 2.0789008140563965\n",
      "Epoch [3610/10000], Loss: 2.078841209411621\n",
      "Epoch [3620/10000], Loss: 2.0784525871276855\n",
      "Epoch [3630/10000], Loss: 2.0789425373077393\n",
      "Epoch [3640/10000], Loss: 2.0783286094665527\n",
      "Epoch [3650/10000], Loss: 2.078950881958008\n",
      "Epoch [3660/10000], Loss: 2.0783019065856934\n",
      "Epoch [3670/10000], Loss: 2.0793261528015137\n",
      "Epoch [3680/10000], Loss: 2.0801029205322266\n",
      "Epoch [3690/10000], Loss: 2.078343152999878\n",
      "Epoch [3700/10000], Loss: 2.079838752746582\n",
      "Epoch [3710/10000], Loss: 2.078856945037842\n",
      "Epoch [3720/10000], Loss: 2.0796971321105957\n",
      "Epoch [3730/10000], Loss: 2.079860210418701\n",
      "Epoch [3740/10000], Loss: 2.078744411468506\n",
      "Epoch [3750/10000], Loss: 2.0787417888641357\n",
      "Epoch [3760/10000], Loss: 2.078845262527466\n",
      "Epoch [3770/10000], Loss: 2.078504800796509\n",
      "Epoch [3780/10000], Loss: 2.078176736831665\n",
      "Epoch [3790/10000], Loss: 2.0786590576171875\n",
      "Epoch [3800/10000], Loss: 2.0774571895599365\n",
      "Epoch [3810/10000], Loss: 2.0793111324310303\n",
      "Epoch [3820/10000], Loss: 2.0779953002929688\n",
      "Epoch [3830/10000], Loss: 2.0784387588500977\n",
      "Epoch [3840/10000], Loss: 2.0778186321258545\n",
      "Epoch [3850/10000], Loss: 2.078493118286133\n",
      "Epoch [3860/10000], Loss: 2.079507827758789\n",
      "Epoch [3870/10000], Loss: 2.0793018341064453\n",
      "Epoch [3880/10000], Loss: 2.0795211791992188\n",
      "Epoch [3890/10000], Loss: 2.079101324081421\n",
      "Epoch [3900/10000], Loss: 2.079467296600342\n",
      "Epoch [3910/10000], Loss: 2.079751491546631\n",
      "Epoch [3920/10000], Loss: 2.078784942626953\n",
      "Epoch [3930/10000], Loss: 2.0778579711914062\n",
      "Epoch [3940/10000], Loss: 2.0782790184020996\n",
      "Epoch [3950/10000], Loss: 2.07857084274292\n",
      "Epoch [3960/10000], Loss: 2.0782551765441895\n",
      "Epoch [3970/10000], Loss: 2.0779383182525635\n",
      "Epoch [3980/10000], Loss: 2.0790529251098633\n",
      "Epoch [3990/10000], Loss: 2.078535556793213\n",
      "Epoch [4000/10000], Loss: 2.079087257385254\n",
      "Epoch [4010/10000], Loss: 2.078789472579956\n",
      "Epoch [4020/10000], Loss: 2.079409599304199\n",
      "Epoch [4030/10000], Loss: 2.077594757080078\n",
      "Epoch [4040/10000], Loss: 2.079507350921631\n",
      "Epoch [4050/10000], Loss: 2.0786495208740234\n",
      "Epoch [4060/10000], Loss: 2.0797691345214844\n",
      "Epoch [4070/10000], Loss: 2.0793468952178955\n",
      "Epoch [4080/10000], Loss: 2.078404664993286\n",
      "Epoch [4090/10000], Loss: 2.0791516304016113\n",
      "Epoch [4100/10000], Loss: 2.0784881114959717\n",
      "Epoch [4110/10000], Loss: 2.0792956352233887\n",
      "Epoch [4120/10000], Loss: 2.0793423652648926\n",
      "Epoch [4130/10000], Loss: 2.07891845703125\n",
      "Epoch [4140/10000], Loss: 2.078779697418213\n",
      "Epoch [4150/10000], Loss: 2.0787477493286133\n",
      "Epoch [4160/10000], Loss: 2.0791406631469727\n",
      "Epoch [4170/10000], Loss: 2.078983783721924\n",
      "Epoch [4180/10000], Loss: 2.0788767337799072\n",
      "Epoch [4190/10000], Loss: 2.0787582397460938\n",
      "Epoch [4200/10000], Loss: 2.078603506088257\n",
      "Epoch [4210/10000], Loss: 2.0789952278137207\n",
      "Epoch [4220/10000], Loss: 2.0795743465423584\n",
      "Epoch [4230/10000], Loss: 2.0782575607299805\n",
      "Epoch [4240/10000], Loss: 2.0786092281341553\n",
      "Epoch [4250/10000], Loss: 2.0779428482055664\n",
      "Epoch [4260/10000], Loss: 2.079049587249756\n",
      "Epoch [4270/10000], Loss: 2.079268455505371\n",
      "Epoch [4280/10000], Loss: 2.0784451961517334\n",
      "Epoch [4290/10000], Loss: 2.078355312347412\n",
      "Epoch [4300/10000], Loss: 2.0777640342712402\n",
      "Epoch [4310/10000], Loss: 2.0784082412719727\n",
      "Epoch [4320/10000], Loss: 2.078991174697876\n",
      "Epoch [4330/10000], Loss: 2.07877254486084\n",
      "Epoch [4340/10000], Loss: 2.0785443782806396\n",
      "Epoch [4350/10000], Loss: 2.078828811645508\n",
      "Epoch [4360/10000], Loss: 2.0781214237213135\n",
      "Epoch [4370/10000], Loss: 2.078701972961426\n",
      "Epoch [4380/10000], Loss: 2.078864336013794\n",
      "Epoch [4390/10000], Loss: 2.079334259033203\n",
      "Epoch [4400/10000], Loss: 2.0791401863098145\n",
      "Epoch [4410/10000], Loss: 2.079568386077881\n",
      "Epoch [4420/10000], Loss: 2.0785908699035645\n",
      "Epoch [4430/10000], Loss: 2.0794179439544678\n",
      "Epoch [4440/10000], Loss: 2.079751491546631\n",
      "Epoch [4450/10000], Loss: 2.0790674686431885\n",
      "Epoch [4460/10000], Loss: 2.079528331756592\n",
      "Epoch [4470/10000], Loss: 2.0784263610839844\n",
      "Epoch [4480/10000], Loss: 2.0786736011505127\n",
      "Epoch [4490/10000], Loss: 2.079186201095581\n",
      "Epoch [4500/10000], Loss: 2.0788259506225586\n",
      "Epoch [4510/10000], Loss: 2.0794222354888916\n",
      "Epoch [4520/10000], Loss: 2.0781402587890625\n",
      "Epoch [4530/10000], Loss: 2.0791540145874023\n",
      "Epoch [4540/10000], Loss: 2.0796327590942383\n",
      "Epoch [4550/10000], Loss: 2.0794944763183594\n",
      "Epoch [4560/10000], Loss: 2.0795230865478516\n",
      "Epoch [4570/10000], Loss: 2.0787391662597656\n",
      "Epoch [4580/10000], Loss: 2.0789499282836914\n",
      "Epoch [4590/10000], Loss: 2.078603506088257\n",
      "Epoch [4600/10000], Loss: 2.0778677463531494\n",
      "Epoch [4610/10000], Loss: 2.0793211460113525\n",
      "Epoch [4620/10000], Loss: 2.0784995555877686\n",
      "Epoch [4630/10000], Loss: 2.0790064334869385\n",
      "Epoch [4640/10000], Loss: 2.078291416168213\n",
      "Epoch [4650/10000], Loss: 2.0780720710754395\n",
      "Epoch [4660/10000], Loss: 2.07930588722229\n",
      "Epoch [4670/10000], Loss: 2.0792391300201416\n",
      "Epoch [4680/10000], Loss: 2.0792603492736816\n",
      "Epoch [4690/10000], Loss: 2.078597068786621\n",
      "Epoch [4700/10000], Loss: 2.079430103302002\n",
      "Epoch [4710/10000], Loss: 2.079132556915283\n",
      "Epoch [4720/10000], Loss: 2.0792019367218018\n",
      "Epoch [4730/10000], Loss: 2.0780744552612305\n",
      "Epoch [4740/10000], Loss: 2.078718423843384\n",
      "Epoch [4750/10000], Loss: 2.078368663787842\n",
      "Epoch [4760/10000], Loss: 2.0781126022338867\n",
      "Epoch [4770/10000], Loss: 2.080200433731079\n",
      "Epoch [4780/10000], Loss: 2.078552722930908\n",
      "Epoch [4790/10000], Loss: 2.0787301063537598\n",
      "Epoch [4800/10000], Loss: 2.0787575244903564\n",
      "Epoch [4810/10000], Loss: 2.079176425933838\n",
      "Epoch [4820/10000], Loss: 2.0788192749023438\n",
      "Epoch [4830/10000], Loss: 2.079272508621216\n",
      "Epoch [4840/10000], Loss: 2.079028844833374\n",
      "Epoch [4850/10000], Loss: 2.0797219276428223\n",
      "Epoch [4860/10000], Loss: 2.079042911529541\n",
      "Epoch [4870/10000], Loss: 2.0787742137908936\n",
      "Epoch [4880/10000], Loss: 2.078559398651123\n",
      "Epoch [4890/10000], Loss: 2.079122304916382\n",
      "Epoch [4900/10000], Loss: 2.0792722702026367\n",
      "Epoch [4910/10000], Loss: 2.079341173171997\n",
      "Epoch [4920/10000], Loss: 2.079181432723999\n",
      "Epoch [4930/10000], Loss: 2.079625368118286\n",
      "Epoch [4940/10000], Loss: 2.078444480895996\n",
      "Epoch [4950/10000], Loss: 2.077913284301758\n",
      "Epoch [4960/10000], Loss: 2.0786571502685547\n",
      "Epoch [4970/10000], Loss: 2.0794992446899414\n",
      "Epoch [4980/10000], Loss: 2.079862594604492\n",
      "Epoch [4990/10000], Loss: 2.0795814990997314\n",
      "Epoch [5000/10000], Loss: 2.0796356201171875\n",
      "Epoch [5010/10000], Loss: 2.078829765319824\n",
      "Epoch [5020/10000], Loss: 2.0800421237945557\n",
      "Epoch [5030/10000], Loss: 2.078690767288208\n",
      "Epoch [5040/10000], Loss: 2.0794456005096436\n",
      "Epoch [5050/10000], Loss: 2.079057455062866\n",
      "Epoch [5060/10000], Loss: 2.079951047897339\n",
      "Epoch [5070/10000], Loss: 2.078907012939453\n",
      "Epoch [5080/10000], Loss: 2.0795390605926514\n",
      "Epoch [5090/10000], Loss: 2.0791141986846924\n",
      "Epoch [5100/10000], Loss: 2.0793638229370117\n",
      "Epoch [5110/10000], Loss: 2.07840895652771\n",
      "Epoch [5120/10000], Loss: 2.0787901878356934\n",
      "Epoch [5130/10000], Loss: 2.0790200233459473\n",
      "Epoch [5140/10000], Loss: 2.078019380569458\n",
      "Epoch [5150/10000], Loss: 2.078397274017334\n",
      "Epoch [5160/10000], Loss: 2.078610897064209\n",
      "Epoch [5170/10000], Loss: 2.079897880554199\n",
      "Epoch [5180/10000], Loss: 2.079512119293213\n",
      "Epoch [5190/10000], Loss: 2.07954740524292\n",
      "Epoch [5200/10000], Loss: 2.078892230987549\n",
      "Epoch [5210/10000], Loss: 2.0783979892730713\n",
      "Epoch [5220/10000], Loss: 2.07873272895813\n",
      "Epoch [5230/10000], Loss: 2.078845262527466\n",
      "Epoch [5240/10000], Loss: 2.0788495540618896\n",
      "Epoch [5250/10000], Loss: 2.0778231620788574\n",
      "Epoch [5260/10000], Loss: 2.0796959400177\n",
      "Epoch [5270/10000], Loss: 2.079399585723877\n",
      "Epoch [5280/10000], Loss: 2.0790398120880127\n",
      "Epoch [5290/10000], Loss: 2.0790326595306396\n",
      "Epoch [5300/10000], Loss: 2.079503059387207\n",
      "Epoch [5310/10000], Loss: 2.079758644104004\n",
      "Epoch [5320/10000], Loss: 2.079709053039551\n",
      "Epoch [5330/10000], Loss: 2.0792949199676514\n",
      "Epoch [5340/10000], Loss: 2.0799179077148438\n",
      "Epoch [5350/10000], Loss: 2.079712152481079\n",
      "Epoch [5360/10000], Loss: 2.0797507762908936\n",
      "Epoch [5370/10000], Loss: 2.0799245834350586\n",
      "Epoch [5380/10000], Loss: 2.078618049621582\n",
      "Epoch [5390/10000], Loss: 2.0790367126464844\n",
      "Epoch [5400/10000], Loss: 2.0792922973632812\n",
      "Epoch [5410/10000], Loss: 2.078205108642578\n",
      "Epoch [5420/10000], Loss: 2.079937219619751\n",
      "Epoch [5430/10000], Loss: 2.079054832458496\n",
      "Epoch [5440/10000], Loss: 2.079141855239868\n",
      "Epoch [5450/10000], Loss: 2.078220844268799\n",
      "Epoch [5460/10000], Loss: 2.079085111618042\n",
      "Epoch [5470/10000], Loss: 2.0795092582702637\n",
      "Epoch [5480/10000], Loss: 2.0792107582092285\n",
      "Epoch [5490/10000], Loss: 2.079076051712036\n",
      "Epoch [5500/10000], Loss: 2.0789735317230225\n",
      "Epoch [5510/10000], Loss: 2.079594135284424\n",
      "Epoch [5520/10000], Loss: 2.078765630722046\n",
      "Epoch [5530/10000], Loss: 2.078911781311035\n",
      "Epoch [5540/10000], Loss: 2.0791852474212646\n",
      "Epoch [5550/10000], Loss: 2.079681873321533\n",
      "Epoch [5560/10000], Loss: 2.080540180206299\n",
      "Epoch [5570/10000], Loss: 2.079622745513916\n",
      "Epoch [5580/10000], Loss: 2.0792455673217773\n",
      "Epoch [5590/10000], Loss: 2.0798189640045166\n",
      "Epoch [5600/10000], Loss: 2.0791587829589844\n",
      "Epoch [5610/10000], Loss: 2.0784664154052734\n",
      "Epoch [5620/10000], Loss: 2.0786476135253906\n",
      "Epoch [5630/10000], Loss: 2.0791735649108887\n",
      "Epoch [5640/10000], Loss: 2.078709125518799\n",
      "Epoch [5650/10000], Loss: 2.0797574520111084\n",
      "Epoch [5660/10000], Loss: 2.078885555267334\n",
      "Epoch [5670/10000], Loss: 2.078813076019287\n",
      "Epoch [5680/10000], Loss: 2.0790340900421143\n",
      "Epoch [5690/10000], Loss: 2.079011917114258\n",
      "Epoch [5700/10000], Loss: 2.0784664154052734\n",
      "Epoch [5710/10000], Loss: 2.0795047283172607\n",
      "Epoch [5720/10000], Loss: 2.0787980556488037\n",
      "Epoch [5730/10000], Loss: 2.0796737670898438\n",
      "Epoch [5740/10000], Loss: 2.0789897441864014\n",
      "Epoch [5750/10000], Loss: 2.078814744949341\n",
      "Epoch [5760/10000], Loss: 2.0791714191436768\n",
      "Epoch [5770/10000], Loss: 2.079899311065674\n",
      "Epoch [5780/10000], Loss: 2.078549385070801\n",
      "Epoch [5790/10000], Loss: 2.0794217586517334\n",
      "Epoch [5800/10000], Loss: 2.078029155731201\n",
      "Epoch [5810/10000], Loss: 2.077876091003418\n",
      "Epoch [5820/10000], Loss: 2.079145908355713\n",
      "Epoch [5830/10000], Loss: 2.0798146724700928\n",
      "Epoch [5840/10000], Loss: 2.078763961791992\n",
      "Epoch [5850/10000], Loss: 2.0800065994262695\n",
      "Epoch [5860/10000], Loss: 2.0798473358154297\n",
      "Epoch [5870/10000], Loss: 2.079545736312866\n",
      "Epoch [5880/10000], Loss: 2.079275369644165\n",
      "Epoch [5890/10000], Loss: 2.079163074493408\n",
      "Epoch [5900/10000], Loss: 2.0788068771362305\n",
      "Epoch [5910/10000], Loss: 2.079909324645996\n",
      "Epoch [5920/10000], Loss: 2.07965087890625\n",
      "Epoch [5930/10000], Loss: 2.079237937927246\n",
      "Epoch [5940/10000], Loss: 2.0787723064422607\n",
      "Epoch [5950/10000], Loss: 2.0790915489196777\n",
      "Epoch [5960/10000], Loss: 2.0797243118286133\n",
      "Epoch [5970/10000], Loss: 2.0790274143218994\n",
      "Epoch [5980/10000], Loss: 2.0793049335479736\n",
      "Epoch [5990/10000], Loss: 2.0795092582702637\n",
      "Epoch [6000/10000], Loss: 2.078800678253174\n",
      "Epoch [6010/10000], Loss: 2.080122232437134\n",
      "Epoch [6020/10000], Loss: 2.0791234970092773\n",
      "Epoch [6030/10000], Loss: 2.0790939331054688\n",
      "Epoch [6040/10000], Loss: 2.0788354873657227\n",
      "Epoch [6050/10000], Loss: 2.079291820526123\n",
      "Epoch [6060/10000], Loss: 2.0784363746643066\n",
      "Epoch [6070/10000], Loss: 2.078521728515625\n",
      "Epoch [6080/10000], Loss: 2.0794172286987305\n",
      "Epoch [6090/10000], Loss: 2.0792126655578613\n",
      "Epoch [6100/10000], Loss: 2.0783891677856445\n",
      "Epoch [6110/10000], Loss: 2.07908296585083\n",
      "Epoch [6120/10000], Loss: 2.0793256759643555\n",
      "Epoch [6130/10000], Loss: 2.0789191722869873\n",
      "Epoch [6140/10000], Loss: 2.079089403152466\n",
      "Epoch [6150/10000], Loss: 2.078808069229126\n",
      "Epoch [6160/10000], Loss: 2.079474449157715\n",
      "Epoch [6170/10000], Loss: 2.079166889190674\n",
      "Epoch [6180/10000], Loss: 2.079096794128418\n",
      "Epoch [6190/10000], Loss: 2.0783355236053467\n",
      "Epoch [6200/10000], Loss: 2.078795909881592\n",
      "Epoch [6210/10000], Loss: 2.0789148807525635\n",
      "Epoch [6220/10000], Loss: 2.0789291858673096\n",
      "Epoch [6230/10000], Loss: 2.0791027545928955\n",
      "Epoch [6240/10000], Loss: 2.079061269760132\n",
      "Epoch [6250/10000], Loss: 2.0779151916503906\n",
      "Epoch [6260/10000], Loss: 2.0788042545318604\n",
      "Epoch [6270/10000], Loss: 2.0786232948303223\n",
      "Epoch [6280/10000], Loss: 2.079390048980713\n",
      "Epoch [6290/10000], Loss: 2.0791194438934326\n",
      "Epoch [6300/10000], Loss: 2.078646421432495\n",
      "Epoch [6310/10000], Loss: 2.0781807899475098\n",
      "Epoch [6320/10000], Loss: 2.0784003734588623\n",
      "Epoch [6330/10000], Loss: 2.078878402709961\n",
      "Epoch [6340/10000], Loss: 2.0794949531555176\n",
      "Epoch [6350/10000], Loss: 2.078700304031372\n",
      "Epoch [6360/10000], Loss: 2.0789902210235596\n",
      "Epoch [6370/10000], Loss: 2.0793073177337646\n",
      "Epoch [6380/10000], Loss: 2.0781352519989014\n",
      "Epoch [6390/10000], Loss: 2.0789988040924072\n",
      "Epoch [6400/10000], Loss: 2.079249858856201\n",
      "Epoch [6410/10000], Loss: 2.078404426574707\n",
      "Epoch [6420/10000], Loss: 2.07885479927063\n",
      "Epoch [6430/10000], Loss: 2.0786240100860596\n",
      "Epoch [6440/10000], Loss: 2.0792014598846436\n",
      "Epoch [6450/10000], Loss: 2.0792858600616455\n",
      "Epoch [6460/10000], Loss: 2.0790858268737793\n",
      "Epoch [6470/10000], Loss: 2.0785584449768066\n",
      "Epoch [6480/10000], Loss: 2.078789234161377\n",
      "Epoch [6490/10000], Loss: 2.0778722763061523\n",
      "Epoch [6500/10000], Loss: 2.0787832736968994\n",
      "Epoch [6510/10000], Loss: 2.079314708709717\n",
      "Epoch [6520/10000], Loss: 2.079475164413452\n",
      "Epoch [6530/10000], Loss: 2.0783987045288086\n",
      "Epoch [6540/10000], Loss: 2.079094648361206\n",
      "Epoch [6550/10000], Loss: 2.0784049034118652\n",
      "Epoch [6560/10000], Loss: 2.0800817012786865\n",
      "Epoch [6570/10000], Loss: 2.0799453258514404\n",
      "Epoch [6580/10000], Loss: 2.0793561935424805\n",
      "Epoch [6590/10000], Loss: 2.0799779891967773\n",
      "Epoch [6600/10000], Loss: 2.079606771469116\n",
      "Epoch [6610/10000], Loss: 2.0796597003936768\n",
      "Epoch [6620/10000], Loss: 2.079733371734619\n",
      "Epoch [6630/10000], Loss: 2.0787320137023926\n",
      "Epoch [6640/10000], Loss: 2.0789313316345215\n",
      "Epoch [6650/10000], Loss: 2.0789794921875\n",
      "Epoch [6660/10000], Loss: 2.0793633460998535\n",
      "Epoch [6670/10000], Loss: 2.0803890228271484\n",
      "Epoch [6680/10000], Loss: 2.079525947570801\n",
      "Epoch [6690/10000], Loss: 2.079281806945801\n",
      "Epoch [6700/10000], Loss: 2.0791361331939697\n",
      "Epoch [6710/10000], Loss: 2.0787668228149414\n",
      "Epoch [6720/10000], Loss: 2.0785446166992188\n",
      "Epoch [6730/10000], Loss: 2.0789783000946045\n",
      "Epoch [6740/10000], Loss: 2.0791144371032715\n",
      "Epoch [6750/10000], Loss: 2.079408884048462\n",
      "Epoch [6760/10000], Loss: 2.080052375793457\n",
      "Epoch [6770/10000], Loss: 2.079646587371826\n",
      "Epoch [6780/10000], Loss: 2.079167604446411\n",
      "Epoch [6790/10000], Loss: 2.0787296295166016\n",
      "Epoch [6800/10000], Loss: 2.0796308517456055\n",
      "Epoch [6810/10000], Loss: 2.0800132751464844\n",
      "Epoch [6820/10000], Loss: 2.0791916847229004\n",
      "Epoch [6830/10000], Loss: 2.078364849090576\n",
      "Epoch [6840/10000], Loss: 2.0791282653808594\n",
      "Epoch [6850/10000], Loss: 2.079479455947876\n",
      "Epoch [6860/10000], Loss: 2.0794129371643066\n",
      "Epoch [6870/10000], Loss: 2.078766345977783\n",
      "Epoch [6880/10000], Loss: 2.0788087844848633\n",
      "Epoch [6890/10000], Loss: 2.0794730186462402\n",
      "Epoch [6900/10000], Loss: 2.0797336101531982\n",
      "Epoch [6910/10000], Loss: 2.077725410461426\n",
      "Epoch [6920/10000], Loss: 2.079087257385254\n",
      "Epoch [6930/10000], Loss: 2.0790727138519287\n",
      "Epoch [6940/10000], Loss: 2.0789356231689453\n",
      "Epoch [6950/10000], Loss: 2.0798473358154297\n",
      "Epoch [6960/10000], Loss: 2.0792527198791504\n",
      "Epoch [6970/10000], Loss: 2.0793538093566895\n",
      "Epoch [6980/10000], Loss: 2.079784631729126\n",
      "Epoch [6990/10000], Loss: 2.0797386169433594\n",
      "Epoch [7000/10000], Loss: 2.0800445079803467\n",
      "Epoch [7010/10000], Loss: 2.0798122882843018\n",
      "Epoch [7020/10000], Loss: 2.0790085792541504\n",
      "Epoch [7030/10000], Loss: 2.078542947769165\n",
      "Epoch [7040/10000], Loss: 2.078925371170044\n",
      "Epoch [7050/10000], Loss: 2.0780234336853027\n",
      "Epoch [7060/10000], Loss: 2.079681158065796\n",
      "Epoch [7070/10000], Loss: 2.0794012546539307\n",
      "Epoch [7080/10000], Loss: 2.079080581665039\n",
      "Epoch [7090/10000], Loss: 2.078779458999634\n",
      "Epoch [7100/10000], Loss: 2.0793092250823975\n",
      "Epoch [7110/10000], Loss: 2.079252004623413\n",
      "Epoch [7120/10000], Loss: 2.079878091812134\n",
      "Epoch [7130/10000], Loss: 2.0795063972473145\n",
      "Epoch [7140/10000], Loss: 2.0794036388397217\n",
      "Epoch [7150/10000], Loss: 2.0793566703796387\n",
      "Epoch [7160/10000], Loss: 2.0772242546081543\n",
      "Epoch [7170/10000], Loss: 2.078629732131958\n",
      "Epoch [7180/10000], Loss: 2.0792629718780518\n",
      "Epoch [7190/10000], Loss: 2.0796689987182617\n",
      "Epoch [7200/10000], Loss: 2.0789949893951416\n",
      "Epoch [7210/10000], Loss: 2.079602003097534\n",
      "Epoch [7220/10000], Loss: 2.0794763565063477\n",
      "Epoch [7230/10000], Loss: 2.0785675048828125\n",
      "Epoch [7240/10000], Loss: 2.079307794570923\n",
      "Epoch [7250/10000], Loss: 2.0788533687591553\n",
      "Epoch [7260/10000], Loss: 2.08019757270813\n",
      "Epoch [7270/10000], Loss: 2.0795412063598633\n",
      "Epoch [7280/10000], Loss: 2.0793280601501465\n",
      "Epoch [7290/10000], Loss: 2.0792717933654785\n",
      "Epoch [7300/10000], Loss: 2.079692840576172\n",
      "Epoch [7310/10000], Loss: 2.0794715881347656\n",
      "Epoch [7320/10000], Loss: 2.0792722702026367\n",
      "Epoch [7330/10000], Loss: 2.079049825668335\n",
      "Epoch [7340/10000], Loss: 2.0793681144714355\n",
      "Epoch [7350/10000], Loss: 2.0795352458953857\n",
      "Epoch [7360/10000], Loss: 2.0790634155273438\n",
      "Epoch [7370/10000], Loss: 2.080380439758301\n",
      "Epoch [7380/10000], Loss: 2.079000949859619\n",
      "Epoch [7390/10000], Loss: 2.0790064334869385\n",
      "Epoch [7400/10000], Loss: 2.079256057739258\n",
      "Epoch [7410/10000], Loss: 2.0786678791046143\n",
      "Epoch [7420/10000], Loss: 2.07956862449646\n",
      "Epoch [7430/10000], Loss: 2.079556465148926\n",
      "Epoch [7440/10000], Loss: 2.078873634338379\n",
      "Epoch [7450/10000], Loss: 2.0788521766662598\n",
      "Epoch [7460/10000], Loss: 2.0792341232299805\n",
      "Epoch [7470/10000], Loss: 2.0792856216430664\n",
      "Epoch [7480/10000], Loss: 2.079742193222046\n",
      "Epoch [7490/10000], Loss: 2.0795319080352783\n",
      "Epoch [7500/10000], Loss: 2.0790839195251465\n",
      "Epoch [7510/10000], Loss: 2.0789742469787598\n",
      "Epoch [7520/10000], Loss: 2.0797765254974365\n",
      "Epoch [7530/10000], Loss: 2.0790016651153564\n",
      "Epoch [7540/10000], Loss: 2.0783495903015137\n",
      "Epoch [7550/10000], Loss: 2.0787513256073\n",
      "Epoch [7560/10000], Loss: 2.0782437324523926\n",
      "Epoch [7570/10000], Loss: 2.0794291496276855\n",
      "Epoch [7580/10000], Loss: 2.0792555809020996\n",
      "Epoch [7590/10000], Loss: 2.079143762588501\n",
      "Epoch [7600/10000], Loss: 2.0798611640930176\n",
      "Epoch [7610/10000], Loss: 2.078671455383301\n",
      "Epoch [7620/10000], Loss: 2.0792043209075928\n",
      "Epoch [7630/10000], Loss: 2.079462766647339\n",
      "Epoch [7640/10000], Loss: 2.079926013946533\n",
      "Epoch [7650/10000], Loss: 2.0804800987243652\n",
      "Epoch [7660/10000], Loss: 2.079333782196045\n",
      "Epoch [7670/10000], Loss: 2.0797362327575684\n",
      "Epoch [7680/10000], Loss: 2.07857346534729\n",
      "Epoch [7690/10000], Loss: 2.078742027282715\n",
      "Epoch [7700/10000], Loss: 2.0792698860168457\n",
      "Epoch [7710/10000], Loss: 2.079488754272461\n",
      "Epoch [7720/10000], Loss: 2.078937530517578\n",
      "Epoch [7730/10000], Loss: 2.078476905822754\n",
      "Epoch [7740/10000], Loss: 2.079383611679077\n",
      "Epoch [7750/10000], Loss: 2.078778028488159\n",
      "Epoch [7760/10000], Loss: 2.0786795616149902\n",
      "Epoch [7770/10000], Loss: 2.079275608062744\n",
      "Epoch [7780/10000], Loss: 2.0785422325134277\n",
      "Epoch [7790/10000], Loss: 2.079383134841919\n",
      "Epoch [7800/10000], Loss: 2.0797433853149414\n",
      "Epoch [7810/10000], Loss: 2.0798144340515137\n",
      "Epoch [7820/10000], Loss: 2.0794506072998047\n",
      "Epoch [7830/10000], Loss: 2.079629898071289\n",
      "Epoch [7840/10000], Loss: 2.0789318084716797\n",
      "Epoch [7850/10000], Loss: 2.0799880027770996\n",
      "Epoch [7860/10000], Loss: 2.079563617706299\n",
      "Epoch [7870/10000], Loss: 2.0802276134490967\n",
      "Epoch [7880/10000], Loss: 2.0795936584472656\n",
      "Epoch [7890/10000], Loss: 2.079725980758667\n",
      "Epoch [7900/10000], Loss: 2.0800023078918457\n",
      "Epoch [7910/10000], Loss: 2.079524278640747\n",
      "Epoch [7920/10000], Loss: 2.0801148414611816\n",
      "Epoch [7930/10000], Loss: 2.07913875579834\n",
      "Epoch [7940/10000], Loss: 2.079341173171997\n",
      "Epoch [7950/10000], Loss: 2.078376293182373\n",
      "Epoch [7960/10000], Loss: 2.0795390605926514\n",
      "Epoch [7970/10000], Loss: 2.0787675380706787\n",
      "Epoch [7980/10000], Loss: 2.078181266784668\n",
      "Epoch [7990/10000], Loss: 2.078566312789917\n",
      "Epoch [8000/10000], Loss: 2.079271078109741\n",
      "Epoch [8010/10000], Loss: 2.079918622970581\n",
      "Epoch [8020/10000], Loss: 2.0781264305114746\n",
      "Epoch [8030/10000], Loss: 2.0795984268188477\n",
      "Epoch [8040/10000], Loss: 2.07930588722229\n",
      "Epoch [8050/10000], Loss: 2.078770160675049\n",
      "Epoch [8060/10000], Loss: 2.079984426498413\n",
      "Epoch [8070/10000], Loss: 2.079242706298828\n",
      "Epoch [8080/10000], Loss: 2.077798366546631\n",
      "Epoch [8090/10000], Loss: 2.080118179321289\n",
      "Epoch [8100/10000], Loss: 2.0799715518951416\n",
      "Epoch [8110/10000], Loss: 2.0793521404266357\n",
      "Epoch [8120/10000], Loss: 2.0785746574401855\n",
      "Epoch [8130/10000], Loss: 2.0799105167388916\n",
      "Epoch [8140/10000], Loss: 2.0787339210510254\n",
      "Epoch [8150/10000], Loss: 2.0788445472717285\n",
      "Epoch [8160/10000], Loss: 2.0790348052978516\n",
      "Epoch [8170/10000], Loss: 2.0798254013061523\n",
      "Epoch [8180/10000], Loss: 2.079166889190674\n",
      "Epoch [8190/10000], Loss: 2.0793745517730713\n",
      "Epoch [8200/10000], Loss: 2.0791735649108887\n",
      "Epoch [8210/10000], Loss: 2.0802464485168457\n",
      "Epoch [8220/10000], Loss: 2.0805115699768066\n",
      "Epoch [8230/10000], Loss: 2.0794119834899902\n",
      "Epoch [8240/10000], Loss: 2.079861640930176\n",
      "Epoch [8250/10000], Loss: 2.0790746212005615\n",
      "Epoch [8260/10000], Loss: 2.0792157649993896\n",
      "Epoch [8270/10000], Loss: 2.079735279083252\n",
      "Epoch [8280/10000], Loss: 2.07995867729187\n",
      "Epoch [8290/10000], Loss: 2.078913450241089\n",
      "Epoch [8300/10000], Loss: 2.079833984375\n",
      "Epoch [8310/10000], Loss: 2.0792312622070312\n",
      "Epoch [8320/10000], Loss: 2.0792176723480225\n",
      "Epoch [8330/10000], Loss: 2.078979969024658\n",
      "Epoch [8340/10000], Loss: 2.0792763233184814\n",
      "Epoch [8350/10000], Loss: 2.078970193862915\n",
      "Epoch [8360/10000], Loss: 2.0789074897766113\n",
      "Epoch [8370/10000], Loss: 2.0794849395751953\n",
      "Epoch [8380/10000], Loss: 2.080228805541992\n",
      "Epoch [8390/10000], Loss: 2.078766345977783\n",
      "Epoch [8400/10000], Loss: 2.079186201095581\n",
      "Epoch [8410/10000], Loss: 2.0784912109375\n",
      "Epoch [8420/10000], Loss: 2.0780227184295654\n",
      "Epoch [8430/10000], Loss: 2.079026699066162\n",
      "Epoch [8440/10000], Loss: 2.078862428665161\n",
      "Epoch [8450/10000], Loss: 2.07861065864563\n",
      "Epoch [8460/10000], Loss: 2.079422950744629\n",
      "Epoch [8470/10000], Loss: 2.0798892974853516\n",
      "Epoch [8480/10000], Loss: 2.0793514251708984\n",
      "Epoch [8490/10000], Loss: 2.079019784927368\n",
      "Epoch [8500/10000], Loss: 2.0788984298706055\n",
      "Epoch [8510/10000], Loss: 2.0791149139404297\n",
      "Epoch [8520/10000], Loss: 2.078582286834717\n",
      "Epoch [8530/10000], Loss: 2.0783252716064453\n",
      "Epoch [8540/10000], Loss: 2.078178882598877\n",
      "Epoch [8550/10000], Loss: 2.0794572830200195\n",
      "Epoch [8560/10000], Loss: 2.0795791149139404\n",
      "Epoch [8570/10000], Loss: 2.079179525375366\n",
      "Epoch [8580/10000], Loss: 2.078944683074951\n",
      "Epoch [8590/10000], Loss: 2.0791337490081787\n",
      "Epoch [8600/10000], Loss: 2.079375982284546\n",
      "Epoch [8610/10000], Loss: 2.0788893699645996\n",
      "Epoch [8620/10000], Loss: 2.0787720680236816\n",
      "Epoch [8630/10000], Loss: 2.078949451446533\n",
      "Epoch [8640/10000], Loss: 2.078569173812866\n",
      "Epoch [8650/10000], Loss: 2.078852415084839\n",
      "Epoch [8660/10000], Loss: 2.0792782306671143\n",
      "Epoch [8670/10000], Loss: 2.0794808864593506\n",
      "Epoch [8680/10000], Loss: 2.0796141624450684\n",
      "Epoch [8690/10000], Loss: 2.077453374862671\n",
      "Epoch [8700/10000], Loss: 2.078402519226074\n",
      "Epoch [8710/10000], Loss: 2.0788915157318115\n",
      "Epoch [8720/10000], Loss: 2.078594446182251\n",
      "Epoch [8730/10000], Loss: 2.0790843963623047\n",
      "Epoch [8740/10000], Loss: 2.078774929046631\n",
      "Epoch [8750/10000], Loss: 2.0794484615325928\n",
      "Epoch [8760/10000], Loss: 2.0798287391662598\n",
      "Epoch [8770/10000], Loss: 2.079005241394043\n",
      "Epoch [8780/10000], Loss: 2.078914165496826\n",
      "Epoch [8790/10000], Loss: 2.0786259174346924\n",
      "Epoch [8800/10000], Loss: 2.0789153575897217\n",
      "Epoch [8810/10000], Loss: 2.0791544914245605\n",
      "Epoch [8820/10000], Loss: 2.0790293216705322\n",
      "Epoch [8830/10000], Loss: 2.0784497261047363\n",
      "Epoch [8840/10000], Loss: 2.0789976119995117\n",
      "Epoch [8850/10000], Loss: 2.0788145065307617\n",
      "Epoch [8860/10000], Loss: 2.0789105892181396\n",
      "Epoch [8870/10000], Loss: 2.079010486602783\n",
      "Epoch [8880/10000], Loss: 2.0787394046783447\n",
      "Epoch [8890/10000], Loss: 2.0787672996520996\n",
      "Epoch [8900/10000], Loss: 2.079820156097412\n",
      "Epoch [8910/10000], Loss: 2.0788395404815674\n",
      "Epoch [8920/10000], Loss: 2.079368829727173\n",
      "Epoch [8930/10000], Loss: 2.078658103942871\n",
      "Epoch [8940/10000], Loss: 2.078601837158203\n",
      "Epoch [8950/10000], Loss: 2.0783705711364746\n",
      "Epoch [8960/10000], Loss: 2.0795657634735107\n",
      "Epoch [8970/10000], Loss: 2.0779874324798584\n",
      "Epoch [8980/10000], Loss: 2.078195333480835\n",
      "Epoch [8990/10000], Loss: 2.0787734985351562\n",
      "Epoch [9000/10000], Loss: 2.0793564319610596\n",
      "Epoch [9010/10000], Loss: 2.0786731243133545\n",
      "Epoch [9020/10000], Loss: 2.0789530277252197\n",
      "Epoch [9030/10000], Loss: 2.0796186923980713\n",
      "Epoch [9040/10000], Loss: 2.079958438873291\n",
      "Epoch [9050/10000], Loss: 2.0792770385742188\n",
      "Epoch [9060/10000], Loss: 2.078188180923462\n",
      "Epoch [9070/10000], Loss: 2.0794687271118164\n",
      "Epoch [9080/10000], Loss: 2.0784339904785156\n",
      "Epoch [9090/10000], Loss: 2.079327344894409\n",
      "Epoch [9100/10000], Loss: 2.0792341232299805\n",
      "Epoch [9110/10000], Loss: 2.0797815322875977\n",
      "Epoch [9120/10000], Loss: 2.0793616771698\n",
      "Epoch [9130/10000], Loss: 2.0786564350128174\n",
      "Epoch [9140/10000], Loss: 2.078465461730957\n",
      "Epoch [9150/10000], Loss: 2.0783677101135254\n",
      "Epoch [9160/10000], Loss: 2.0799508094787598\n",
      "Epoch [9170/10000], Loss: 2.0807862281799316\n",
      "Epoch [9180/10000], Loss: 2.079442024230957\n",
      "Epoch [9190/10000], Loss: 2.079897880554199\n",
      "Epoch [9200/10000], Loss: 2.079155206680298\n",
      "Epoch [9210/10000], Loss: 2.0802247524261475\n",
      "Epoch [9220/10000], Loss: 2.079516887664795\n",
      "Epoch [9230/10000], Loss: 2.0803685188293457\n",
      "Epoch [9240/10000], Loss: 2.079932451248169\n",
      "Epoch [9250/10000], Loss: 2.0786094665527344\n",
      "Epoch [9260/10000], Loss: 2.0787510871887207\n",
      "Epoch [9270/10000], Loss: 2.0790185928344727\n",
      "Epoch [9280/10000], Loss: 2.0784454345703125\n",
      "Epoch [9290/10000], Loss: 2.0794763565063477\n",
      "Epoch [9300/10000], Loss: 2.079453945159912\n",
      "Epoch [9310/10000], Loss: 2.07898211479187\n",
      "Epoch [9320/10000], Loss: 2.0800328254699707\n",
      "Epoch [9330/10000], Loss: 2.079698324203491\n",
      "Epoch [9340/10000], Loss: 2.08001446723938\n",
      "Epoch [9350/10000], Loss: 2.0801689624786377\n",
      "Epoch [9360/10000], Loss: 2.079256296157837\n",
      "Epoch [9370/10000], Loss: 2.0795109272003174\n",
      "Epoch [9380/10000], Loss: 2.0788543224334717\n",
      "Epoch [9390/10000], Loss: 2.0792202949523926\n",
      "Epoch [9400/10000], Loss: 2.079162836074829\n",
      "Epoch [9410/10000], Loss: 2.0789074897766113\n",
      "Epoch [9420/10000], Loss: 2.078939914703369\n",
      "Epoch [9430/10000], Loss: 2.0787479877471924\n",
      "Epoch [9440/10000], Loss: 2.0788092613220215\n",
      "Epoch [9450/10000], Loss: 2.0794384479522705\n",
      "Epoch [9460/10000], Loss: 2.078883409500122\n",
      "Epoch [9470/10000], Loss: 2.0790653228759766\n",
      "Epoch [9480/10000], Loss: 2.0790796279907227\n",
      "Epoch [9490/10000], Loss: 2.078645706176758\n",
      "Epoch [9500/10000], Loss: 2.0799341201782227\n",
      "Epoch [9510/10000], Loss: 2.077831745147705\n",
      "Epoch [9520/10000], Loss: 2.078994035720825\n",
      "Epoch [9530/10000], Loss: 2.079694986343384\n",
      "Epoch [9540/10000], Loss: 2.0798492431640625\n",
      "Epoch [9550/10000], Loss: 2.0797619819641113\n",
      "Epoch [9560/10000], Loss: 2.0809175968170166\n",
      "Epoch [9570/10000], Loss: 2.0803651809692383\n",
      "Epoch [9580/10000], Loss: 2.0797970294952393\n",
      "Epoch [9590/10000], Loss: 2.079596757888794\n",
      "Epoch [9600/10000], Loss: 2.079754590988159\n",
      "Epoch [9610/10000], Loss: 2.079880714416504\n",
      "Epoch [9620/10000], Loss: 2.07867431640625\n",
      "Epoch [9630/10000], Loss: 2.0810554027557373\n",
      "Epoch [9640/10000], Loss: 2.078502655029297\n",
      "Epoch [9650/10000], Loss: 2.0784645080566406\n",
      "Epoch [9660/10000], Loss: 2.079341173171997\n",
      "Epoch [9670/10000], Loss: 2.0789129734039307\n",
      "Epoch [9680/10000], Loss: 2.0785036087036133\n",
      "Epoch [9690/10000], Loss: 2.0783586502075195\n",
      "Epoch [9700/10000], Loss: 2.0790696144104004\n",
      "Epoch [9710/10000], Loss: 2.0785298347473145\n",
      "Epoch [9720/10000], Loss: 2.077970504760742\n",
      "Epoch [9730/10000], Loss: 2.0786685943603516\n",
      "Epoch [9740/10000], Loss: 2.079557418823242\n",
      "Epoch [9750/10000], Loss: 2.0786116123199463\n",
      "Epoch [9760/10000], Loss: 2.078983783721924\n",
      "Epoch [9770/10000], Loss: 2.07967472076416\n",
      "Epoch [9780/10000], Loss: 2.07924222946167\n",
      "Epoch [9790/10000], Loss: 2.079345703125\n",
      "Epoch [9800/10000], Loss: 2.080085277557373\n",
      "Epoch [9810/10000], Loss: 2.0787339210510254\n",
      "Epoch [9820/10000], Loss: 2.079209089279175\n",
      "Epoch [9830/10000], Loss: 2.078878402709961\n",
      "Epoch [9840/10000], Loss: 2.078695774078369\n",
      "Epoch [9850/10000], Loss: 2.0786759853363037\n",
      "Epoch [9860/10000], Loss: 2.0784332752227783\n",
      "Epoch [9870/10000], Loss: 2.0797274112701416\n",
      "Epoch [9880/10000], Loss: 2.0788185596466064\n",
      "Epoch [9890/10000], Loss: 2.0788145065307617\n",
      "Epoch [9900/10000], Loss: 2.0791568756103516\n",
      "Epoch [9910/10000], Loss: 2.0792646408081055\n",
      "Epoch [9920/10000], Loss: 2.0797996520996094\n",
      "Epoch [9930/10000], Loss: 2.080463171005249\n",
      "Epoch [9940/10000], Loss: 2.080425262451172\n",
      "Epoch [9950/10000], Loss: 2.080328941345215\n",
      "Epoch [9960/10000], Loss: 2.078587293624878\n",
      "Epoch [9970/10000], Loss: 2.079648494720459\n",
      "Epoch [9980/10000], Loss: 2.078681707382202\n",
      "Epoch [9990/10000], Loss: 2.0794548988342285\n",
      "Epoch [10000/10000], Loss: 2.0787715911865234\n",
      "Epoch [10/10000], Loss: 50.297142028808594\n",
      "Epoch [20/10000], Loss: 39.00840759277344\n",
      "Epoch [30/10000], Loss: 29.58606719970703\n",
      "Epoch [40/10000], Loss: 23.848812103271484\n",
      "Epoch [50/10000], Loss: 19.887733459472656\n",
      "Epoch [60/10000], Loss: 16.430156707763672\n",
      "Epoch [70/10000], Loss: 13.779949188232422\n",
      "Epoch [80/10000], Loss: 11.28875732421875\n",
      "Epoch [90/10000], Loss: 10.04455280303955\n",
      "Epoch [100/10000], Loss: 9.339837074279785\n",
      "Epoch [110/10000], Loss: 8.561646461486816\n",
      "Epoch [120/10000], Loss: 8.14781665802002\n",
      "Epoch [130/10000], Loss: 7.954534530639648\n",
      "Epoch [140/10000], Loss: 7.810851097106934\n",
      "Epoch [150/10000], Loss: 7.679172515869141\n",
      "Epoch [160/10000], Loss: 7.560646057128906\n",
      "Epoch [170/10000], Loss: 7.466293811798096\n",
      "Epoch [180/10000], Loss: 7.348635196685791\n",
      "Epoch [190/10000], Loss: 7.261448860168457\n",
      "Epoch [200/10000], Loss: 7.034873962402344\n",
      "Epoch [210/10000], Loss: 6.9448041915893555\n",
      "Epoch [220/10000], Loss: 7.0999531745910645\n",
      "Epoch [230/10000], Loss: 6.996720314025879\n",
      "Epoch [240/10000], Loss: 7.140801429748535\n",
      "Epoch [250/10000], Loss: 7.014021873474121\n",
      "Epoch [260/10000], Loss: 6.897139549255371\n",
      "Epoch [270/10000], Loss: 6.810590744018555\n",
      "Epoch [280/10000], Loss: 6.802980422973633\n",
      "Epoch [290/10000], Loss: 6.683338165283203\n",
      "Epoch [300/10000], Loss: 6.57369327545166\n",
      "Epoch [310/10000], Loss: 6.480493545532227\n",
      "Epoch [320/10000], Loss: 6.374669075012207\n",
      "Epoch [330/10000], Loss: 6.270773410797119\n",
      "Epoch [340/10000], Loss: 6.173003673553467\n",
      "Epoch [350/10000], Loss: 6.067131042480469\n",
      "Epoch [360/10000], Loss: 5.975963115692139\n",
      "Epoch [370/10000], Loss: 5.885284423828125\n",
      "Epoch [380/10000], Loss: 5.661996841430664\n",
      "Epoch [390/10000], Loss: 5.574366569519043\n",
      "Epoch [400/10000], Loss: 5.2145209312438965\n",
      "Epoch [410/10000], Loss: 5.1455769538879395\n",
      "Epoch [420/10000], Loss: 5.091953277587891\n",
      "Epoch [430/10000], Loss: 5.042229652404785\n",
      "Epoch [440/10000], Loss: 5.002717018127441\n",
      "Epoch [450/10000], Loss: 4.953067779541016\n",
      "Epoch [460/10000], Loss: 4.907599925994873\n",
      "Epoch [470/10000], Loss: 4.868435382843018\n",
      "Epoch [480/10000], Loss: 5.069513320922852\n",
      "Epoch [490/10000], Loss: 5.027349948883057\n",
      "Epoch [500/10000], Loss: 4.817522048950195\n",
      "Epoch [510/10000], Loss: 4.764035224914551\n",
      "Epoch [520/10000], Loss: 4.701688289642334\n",
      "Epoch [530/10000], Loss: 4.656233310699463\n",
      "Epoch [540/10000], Loss: 4.610604286193848\n",
      "Epoch [550/10000], Loss: 4.553384780883789\n",
      "Epoch [560/10000], Loss: 4.508566856384277\n",
      "Epoch [570/10000], Loss: 4.480594635009766\n",
      "Epoch [580/10000], Loss: 4.4440412521362305\n",
      "Epoch [590/10000], Loss: 4.39310359954834\n",
      "Epoch [600/10000], Loss: 4.365671157836914\n",
      "Epoch [610/10000], Loss: 4.314903259277344\n",
      "Epoch [620/10000], Loss: 4.158506393432617\n",
      "Epoch [630/10000], Loss: 4.137177467346191\n",
      "Epoch [640/10000], Loss: 4.110940456390381\n",
      "Epoch [650/10000], Loss: 4.09462308883667\n",
      "Epoch [660/10000], Loss: 4.078092575073242\n",
      "Epoch [670/10000], Loss: 4.058812141418457\n",
      "Epoch [680/10000], Loss: 4.29467248916626\n",
      "Epoch [690/10000], Loss: 4.27289342880249\n",
      "Epoch [700/10000], Loss: 4.2295379638671875\n",
      "Epoch [710/10000], Loss: 4.2140045166015625\n",
      "Epoch [720/10000], Loss: 4.190776824951172\n",
      "Epoch [730/10000], Loss: 4.1629958152771\n",
      "Epoch [740/10000], Loss: 4.139744758605957\n",
      "Epoch [750/10000], Loss: 4.116710662841797\n",
      "Epoch [760/10000], Loss: 4.104042053222656\n",
      "Epoch [770/10000], Loss: 4.070867538452148\n",
      "Epoch [780/10000], Loss: 4.048312664031982\n",
      "Epoch [790/10000], Loss: 4.023828983306885\n",
      "Epoch [800/10000], Loss: 4.005025863647461\n",
      "Epoch [810/10000], Loss: 3.993492603302002\n",
      "Epoch [820/10000], Loss: 3.9685893058776855\n",
      "Epoch [830/10000], Loss: 3.951869487762451\n",
      "Epoch [840/10000], Loss: 3.940415859222412\n",
      "Epoch [850/10000], Loss: 3.7835803031921387\n",
      "Epoch [860/10000], Loss: 3.773378372192383\n",
      "Epoch [870/10000], Loss: 3.746206283569336\n",
      "Epoch [880/10000], Loss: 3.743777275085449\n",
      "Epoch [890/10000], Loss: 3.7423553466796875\n",
      "Epoch [900/10000], Loss: 3.7282052040100098\n",
      "Epoch [910/10000], Loss: 3.719083786010742\n",
      "Epoch [920/10000], Loss: 3.7100093364715576\n",
      "Epoch [930/10000], Loss: 3.70145583152771\n",
      "Epoch [940/10000], Loss: 3.9420547485351562\n",
      "Epoch [950/10000], Loss: 4.150778770446777\n",
      "Epoch [960/10000], Loss: 4.130038738250732\n",
      "Epoch [970/10000], Loss: 4.063987731933594\n",
      "Epoch [980/10000], Loss: 4.015965461730957\n",
      "Epoch [990/10000], Loss: 3.9705567359924316\n",
      "Epoch [1000/10000], Loss: 3.9124271869659424\n",
      "Epoch [1010/10000], Loss: 3.876126527786255\n",
      "Epoch [1020/10000], Loss: 3.827181100845337\n",
      "Epoch [1030/10000], Loss: 3.7856860160827637\n",
      "Epoch [1040/10000], Loss: 3.741755485534668\n",
      "Epoch [1050/10000], Loss: 3.6915159225463867\n",
      "Epoch [1060/10000], Loss: 3.648069381713867\n",
      "Epoch [1070/10000], Loss: 3.476956844329834\n",
      "Epoch [1080/10000], Loss: 3.439669132232666\n",
      "Epoch [1090/10000], Loss: 3.281879425048828\n",
      "Epoch [1100/10000], Loss: 3.2549400329589844\n",
      "Epoch [1110/10000], Loss: 3.246565818786621\n",
      "Epoch [1120/10000], Loss: 3.2482378482818604\n",
      "Epoch [1130/10000], Loss: 3.23000431060791\n",
      "Epoch [1140/10000], Loss: 3.239623785018921\n",
      "Epoch [1150/10000], Loss: 3.2363719940185547\n",
      "Epoch [1160/10000], Loss: 3.220303535461426\n",
      "Epoch [1170/10000], Loss: 3.212759017944336\n",
      "Epoch [1180/10000], Loss: 3.219481945037842\n",
      "Epoch [1190/10000], Loss: 3.2260091304779053\n",
      "Epoch [1200/10000], Loss: 3.2073721885681152\n",
      "Epoch [1210/10000], Loss: 3.2011427879333496\n",
      "Epoch [1220/10000], Loss: 3.1999757289886475\n",
      "Epoch [1230/10000], Loss: 3.1932780742645264\n",
      "Epoch [1240/10000], Loss: 3.1978070735931396\n",
      "Epoch [1250/10000], Loss: 3.1821212768554688\n",
      "Epoch [1260/10000], Loss: 3.4300594329833984\n",
      "Epoch [1270/10000], Loss: 3.4093759059906006\n",
      "Epoch [1280/10000], Loss: 3.3842897415161133\n",
      "Epoch [1290/10000], Loss: 3.3506908416748047\n",
      "Epoch [1300/10000], Loss: 3.3446431159973145\n",
      "Epoch [1310/10000], Loss: 3.3082103729248047\n",
      "Epoch [1320/10000], Loss: 3.3032760620117188\n",
      "Epoch [1330/10000], Loss: 3.275339365005493\n",
      "Epoch [1340/10000], Loss: 3.25649356842041\n",
      "Epoch [1350/10000], Loss: 3.234032154083252\n",
      "Epoch [1360/10000], Loss: 3.2169642448425293\n",
      "Epoch [1370/10000], Loss: 3.0583226680755615\n",
      "Epoch [1380/10000], Loss: 3.0529000759124756\n",
      "Epoch [1390/10000], Loss: 3.0402870178222656\n",
      "Epoch [1400/10000], Loss: 3.0450985431671143\n",
      "Epoch [1410/10000], Loss: 3.0308024883270264\n",
      "Epoch [1420/10000], Loss: 3.0338025093078613\n",
      "Epoch [1430/10000], Loss: 3.014265298843384\n",
      "Epoch [1440/10000], Loss: 3.0115323066711426\n",
      "Epoch [1450/10000], Loss: 3.024824619293213\n",
      "Epoch [1460/10000], Loss: 3.0177559852600098\n",
      "Epoch [1470/10000], Loss: 3.0223727226257324\n",
      "Epoch [1480/10000], Loss: 3.0124566555023193\n",
      "Epoch [1490/10000], Loss: 3.0086874961853027\n",
      "Epoch [1500/10000], Loss: 3.0105032920837402\n",
      "Epoch [1510/10000], Loss: 2.9959769248962402\n",
      "Epoch [1520/10000], Loss: 3.002997636795044\n",
      "Epoch [1530/10000], Loss: 3.00559139251709\n",
      "Epoch [1540/10000], Loss: 3.0137956142425537\n",
      "Epoch [1550/10000], Loss: 3.0035502910614014\n",
      "Epoch [1560/10000], Loss: 3.0044806003570557\n",
      "Epoch [1570/10000], Loss: 3.0057215690612793\n",
      "Epoch [1580/10000], Loss: 2.993454694747925\n",
      "Epoch [1590/10000], Loss: 3.0050885677337646\n",
      "Epoch [1600/10000], Loss: 3.003911018371582\n",
      "Epoch [1610/10000], Loss: 3.006289005279541\n",
      "Epoch [1620/10000], Loss: 3.0018887519836426\n",
      "Epoch [1630/10000], Loss: 2.9987070560455322\n",
      "Epoch [1640/10000], Loss: 3.0017755031585693\n",
      "Epoch [1650/10000], Loss: 2.9878134727478027\n",
      "Epoch [1660/10000], Loss: 2.997148036956787\n",
      "Epoch [1670/10000], Loss: 3.003169536590576\n",
      "Epoch [1680/10000], Loss: 2.9956231117248535\n",
      "Epoch [1690/10000], Loss: 3.0164613723754883\n",
      "Epoch [1700/10000], Loss: 2.99224853515625\n",
      "Epoch [1710/10000], Loss: 2.9959826469421387\n",
      "Epoch [1720/10000], Loss: 2.994673013687134\n",
      "Epoch [1730/10000], Loss: 2.9914236068725586\n",
      "Epoch [1740/10000], Loss: 2.9910171031951904\n",
      "Epoch [1750/10000], Loss: 2.9956631660461426\n",
      "Epoch [1760/10000], Loss: 2.995776653289795\n",
      "Epoch [1770/10000], Loss: 2.991594076156616\n",
      "Epoch [1780/10000], Loss: 2.9851667881011963\n",
      "Epoch [1790/10000], Loss: 2.987288236618042\n",
      "Epoch [1800/10000], Loss: 2.9921579360961914\n",
      "Epoch [1810/10000], Loss: 2.9859778881073\n",
      "Epoch [1820/10000], Loss: 2.996436595916748\n",
      "Epoch [1830/10000], Loss: 2.9903063774108887\n",
      "Epoch [1840/10000], Loss: 2.9932470321655273\n",
      "Epoch [1850/10000], Loss: 2.9835095405578613\n",
      "Epoch [1860/10000], Loss: 2.9854326248168945\n",
      "Epoch [1870/10000], Loss: 2.994251012802124\n",
      "Epoch [1880/10000], Loss: 2.9849092960357666\n",
      "Epoch [1890/10000], Loss: 2.9895474910736084\n",
      "Epoch [1900/10000], Loss: 2.9995923042297363\n",
      "Epoch [1910/10000], Loss: 2.998037815093994\n",
      "Epoch [1920/10000], Loss: 2.98820161819458\n",
      "Epoch [1930/10000], Loss: 2.9786276817321777\n",
      "Epoch [1940/10000], Loss: 2.984379529953003\n",
      "Epoch [1950/10000], Loss: 2.990764617919922\n",
      "Epoch [1960/10000], Loss: 2.980043888092041\n",
      "Epoch [1970/10000], Loss: 2.9899139404296875\n",
      "Epoch [1980/10000], Loss: 2.9849367141723633\n",
      "Epoch [1990/10000], Loss: 2.998850107192993\n",
      "Epoch [2000/10000], Loss: 2.984710216522217\n",
      "Epoch [2010/10000], Loss: 2.993431568145752\n",
      "Epoch [2020/10000], Loss: 2.999598264694214\n",
      "Epoch [2030/10000], Loss: 2.995755195617676\n",
      "Epoch [2040/10000], Loss: 3.0019755363464355\n",
      "Epoch [2050/10000], Loss: 2.994166374206543\n",
      "Epoch [2060/10000], Loss: 2.9809820652008057\n",
      "Epoch [2070/10000], Loss: 2.9976978302001953\n",
      "Epoch [2080/10000], Loss: 2.9965813159942627\n",
      "Epoch [2090/10000], Loss: 2.9939918518066406\n",
      "Epoch [2100/10000], Loss: 2.991595506668091\n",
      "Epoch [2110/10000], Loss: 2.9952590465545654\n",
      "Epoch [2120/10000], Loss: 2.9938158988952637\n",
      "Epoch [2130/10000], Loss: 3.0009517669677734\n",
      "Epoch [2140/10000], Loss: 3.001491069793701\n",
      "Epoch [2150/10000], Loss: 2.9952988624572754\n",
      "Epoch [2160/10000], Loss: 2.9889798164367676\n",
      "Epoch [2170/10000], Loss: 2.988037109375\n",
      "Epoch [2180/10000], Loss: 2.987150192260742\n",
      "Epoch [2190/10000], Loss: 2.9878716468811035\n",
      "Epoch [2200/10000], Loss: 2.9853577613830566\n",
      "Epoch [2210/10000], Loss: 2.975724458694458\n",
      "Epoch [2220/10000], Loss: 2.9841480255126953\n",
      "Epoch [2230/10000], Loss: 2.9918222427368164\n",
      "Epoch [2240/10000], Loss: 2.9841017723083496\n",
      "Epoch [2250/10000], Loss: 2.9830143451690674\n",
      "Epoch [2260/10000], Loss: 2.9948253631591797\n",
      "Epoch [2270/10000], Loss: 2.9871318340301514\n",
      "Epoch [2280/10000], Loss: 2.9890828132629395\n",
      "Epoch [2290/10000], Loss: 2.979037046432495\n",
      "Epoch [2300/10000], Loss: 2.9879133701324463\n",
      "Epoch [2310/10000], Loss: 2.9797277450561523\n",
      "Epoch [2320/10000], Loss: 2.9937708377838135\n",
      "Epoch [2330/10000], Loss: 2.981233596801758\n",
      "Epoch [2340/10000], Loss: 2.986215353012085\n",
      "Epoch [2350/10000], Loss: 2.9907305240631104\n",
      "Epoch [2360/10000], Loss: 2.9933695793151855\n",
      "Epoch [2370/10000], Loss: 2.983144998550415\n",
      "Epoch [2380/10000], Loss: 2.9959816932678223\n",
      "Epoch [2390/10000], Loss: 2.989377498626709\n",
      "Epoch [2400/10000], Loss: 2.994246482849121\n",
      "Epoch [2410/10000], Loss: 2.990041971206665\n",
      "Epoch [2420/10000], Loss: 2.9835572242736816\n",
      "Epoch [2430/10000], Loss: 2.9907290935516357\n",
      "Epoch [2440/10000], Loss: 2.9884865283966064\n",
      "Epoch [2450/10000], Loss: 2.998436450958252\n",
      "Epoch [2460/10000], Loss: 2.9839842319488525\n",
      "Epoch [2470/10000], Loss: 2.9889779090881348\n",
      "Epoch [2480/10000], Loss: 2.99868106842041\n",
      "Epoch [2490/10000], Loss: 2.9888594150543213\n",
      "Epoch [2500/10000], Loss: 2.996659278869629\n",
      "Epoch [2510/10000], Loss: 2.991244316101074\n",
      "Epoch [2520/10000], Loss: 2.98600172996521\n",
      "Epoch [2530/10000], Loss: 2.9830379486083984\n",
      "Epoch [2540/10000], Loss: 2.9751534461975098\n",
      "Epoch [2550/10000], Loss: 2.985501766204834\n",
      "Epoch [2560/10000], Loss: 2.9826738834381104\n",
      "Epoch [2570/10000], Loss: 2.966663122177124\n",
      "Epoch [2580/10000], Loss: 2.9845221042633057\n",
      "Epoch [2590/10000], Loss: 2.988528251647949\n",
      "Epoch [2600/10000], Loss: 2.9936678409576416\n",
      "Epoch [2610/10000], Loss: 2.985805034637451\n",
      "Epoch [2620/10000], Loss: 2.9855127334594727\n",
      "Epoch [2630/10000], Loss: 2.9888343811035156\n",
      "Epoch [2640/10000], Loss: 2.988276958465576\n",
      "Epoch [2650/10000], Loss: 2.9919843673706055\n",
      "Epoch [2660/10000], Loss: 2.9881412982940674\n",
      "Epoch [2670/10000], Loss: 2.994697093963623\n",
      "Epoch [2680/10000], Loss: 2.9828922748565674\n",
      "Epoch [2690/10000], Loss: 2.973207712173462\n",
      "Epoch [2700/10000], Loss: 2.9894556999206543\n",
      "Epoch [2710/10000], Loss: 2.9904394149780273\n",
      "Epoch [2720/10000], Loss: 2.989363670349121\n",
      "Epoch [2730/10000], Loss: 2.9828171730041504\n",
      "Epoch [2740/10000], Loss: 2.9795479774475098\n",
      "Epoch [2750/10000], Loss: 2.9872453212738037\n",
      "Epoch [2760/10000], Loss: 2.984438419342041\n",
      "Epoch [2770/10000], Loss: 2.98465633392334\n",
      "Epoch [2780/10000], Loss: 2.993014097213745\n",
      "Epoch [2790/10000], Loss: 2.990365505218506\n",
      "Epoch [2800/10000], Loss: 2.9818813800811768\n",
      "Epoch [2810/10000], Loss: 2.987891435623169\n",
      "Epoch [2820/10000], Loss: 2.984013080596924\n",
      "Epoch [2830/10000], Loss: 2.9884517192840576\n",
      "Epoch [2840/10000], Loss: 2.974669933319092\n",
      "Epoch [2850/10000], Loss: 2.9881396293640137\n",
      "Epoch [2860/10000], Loss: 2.978705644607544\n",
      "Epoch [2870/10000], Loss: 2.9875171184539795\n",
      "Epoch [2880/10000], Loss: 2.9988460540771484\n",
      "Epoch [2890/10000], Loss: 2.9935109615325928\n",
      "Epoch [2900/10000], Loss: 3.001079559326172\n",
      "Epoch [2910/10000], Loss: 2.9922051429748535\n",
      "Epoch [2920/10000], Loss: 2.982245445251465\n",
      "Epoch [2930/10000], Loss: 3.0000247955322266\n",
      "Epoch [2940/10000], Loss: 2.981260061264038\n",
      "Epoch [2950/10000], Loss: 2.978454828262329\n",
      "Epoch [2960/10000], Loss: 2.9812679290771484\n",
      "Epoch [2970/10000], Loss: 2.988246440887451\n",
      "Epoch [2980/10000], Loss: 2.9863195419311523\n",
      "Epoch [2990/10000], Loss: 2.9931893348693848\n",
      "Epoch [3000/10000], Loss: 2.9964988231658936\n",
      "Epoch [3010/10000], Loss: 2.9899556636810303\n",
      "Epoch [3020/10000], Loss: 2.9911975860595703\n",
      "Epoch [3030/10000], Loss: 2.9865198135375977\n",
      "Epoch [3040/10000], Loss: 2.999696731567383\n",
      "Epoch [3050/10000], Loss: 2.996076822280884\n",
      "Epoch [3060/10000], Loss: 2.986119508743286\n",
      "Epoch [3070/10000], Loss: 2.984177589416504\n",
      "Epoch [3080/10000], Loss: 2.993421792984009\n",
      "Epoch [3090/10000], Loss: 2.986591339111328\n",
      "Epoch [3100/10000], Loss: 2.9900143146514893\n",
      "Epoch [3110/10000], Loss: 2.993879795074463\n",
      "Epoch [3120/10000], Loss: 2.9837896823883057\n",
      "Epoch [3130/10000], Loss: 2.9939804077148438\n",
      "Epoch [3140/10000], Loss: 2.9843010902404785\n",
      "Epoch [3150/10000], Loss: 2.979731559753418\n",
      "Epoch [3160/10000], Loss: 2.972917318344116\n",
      "Epoch [3170/10000], Loss: 2.9830398559570312\n",
      "Epoch [3180/10000], Loss: 2.9911248683929443\n",
      "Epoch [3190/10000], Loss: 2.9853129386901855\n",
      "Epoch [3200/10000], Loss: 2.9768576622009277\n",
      "Epoch [3210/10000], Loss: 2.990231513977051\n",
      "Epoch [3220/10000], Loss: 2.9785890579223633\n",
      "Epoch [3230/10000], Loss: 2.9827306270599365\n",
      "Epoch [3240/10000], Loss: 2.994690418243408\n",
      "Epoch [3250/10000], Loss: 2.993100643157959\n",
      "Epoch [3260/10000], Loss: 2.9964568614959717\n",
      "Epoch [3270/10000], Loss: 2.990567207336426\n",
      "Epoch [3280/10000], Loss: 2.9926998615264893\n",
      "Epoch [3290/10000], Loss: 2.992403745651245\n",
      "Epoch [3300/10000], Loss: 2.981070041656494\n",
      "Epoch [3310/10000], Loss: 2.9839229583740234\n",
      "Epoch [3320/10000], Loss: 2.9828054904937744\n",
      "Epoch [3330/10000], Loss: 2.981280565261841\n",
      "Epoch [3340/10000], Loss: 2.995244264602661\n",
      "Epoch [3350/10000], Loss: 2.9896183013916016\n",
      "Epoch [3360/10000], Loss: 2.989177942276001\n",
      "Epoch [3370/10000], Loss: 2.994415760040283\n",
      "Epoch [3380/10000], Loss: 2.980167865753174\n",
      "Epoch [3390/10000], Loss: 2.989532470703125\n",
      "Epoch [3400/10000], Loss: 2.9933342933654785\n",
      "Epoch [3410/10000], Loss: 2.992828369140625\n",
      "Epoch [3420/10000], Loss: 2.994223117828369\n",
      "Epoch [3430/10000], Loss: 2.9951531887054443\n",
      "Epoch [3440/10000], Loss: 2.986370325088501\n",
      "Epoch [3450/10000], Loss: 2.9868321418762207\n",
      "Epoch [3460/10000], Loss: 2.988802909851074\n",
      "Epoch [3470/10000], Loss: 2.9772486686706543\n",
      "Epoch [3480/10000], Loss: 2.9965338706970215\n",
      "Epoch [3490/10000], Loss: 2.9861607551574707\n",
      "Epoch [3500/10000], Loss: 2.999082565307617\n",
      "Epoch [3510/10000], Loss: 2.9931483268737793\n",
      "Epoch [3520/10000], Loss: 2.9909636974334717\n",
      "Epoch [3530/10000], Loss: 2.9915380477905273\n",
      "Epoch [3540/10000], Loss: 2.987487316131592\n",
      "Epoch [3550/10000], Loss: 2.9769392013549805\n",
      "Epoch [3560/10000], Loss: 2.9893054962158203\n",
      "Epoch [3570/10000], Loss: 2.9851925373077393\n",
      "Epoch [3580/10000], Loss: 2.9924228191375732\n",
      "Epoch [3590/10000], Loss: 2.9970312118530273\n",
      "Epoch [3600/10000], Loss: 2.9746921062469482\n",
      "Epoch [3610/10000], Loss: 2.9857370853424072\n",
      "Epoch [3620/10000], Loss: 2.990201950073242\n",
      "Epoch [3630/10000], Loss: 2.9914329051971436\n",
      "Epoch [3640/10000], Loss: 2.995490074157715\n",
      "Epoch [3650/10000], Loss: 2.9926767349243164\n",
      "Epoch [3660/10000], Loss: 2.9849863052368164\n",
      "Epoch [3670/10000], Loss: 2.9872286319732666\n",
      "Epoch [3680/10000], Loss: 2.9855949878692627\n",
      "Epoch [3690/10000], Loss: 2.982940673828125\n",
      "Epoch [3700/10000], Loss: 2.9834718704223633\n",
      "Epoch [3710/10000], Loss: 2.998220205307007\n",
      "Epoch [3720/10000], Loss: 2.999128818511963\n",
      "Epoch [3730/10000], Loss: 2.9965500831604004\n",
      "Epoch [3740/10000], Loss: 2.9941177368164062\n",
      "Epoch [3750/10000], Loss: 2.997962474822998\n",
      "Epoch [3760/10000], Loss: 2.9877264499664307\n",
      "Epoch [3770/10000], Loss: 2.986201047897339\n",
      "Epoch [3780/10000], Loss: 2.9945156574249268\n",
      "Epoch [3790/10000], Loss: 2.9844093322753906\n",
      "Epoch [3800/10000], Loss: 2.987678050994873\n",
      "Epoch [3810/10000], Loss: 2.9934568405151367\n",
      "Epoch [3820/10000], Loss: 2.994581699371338\n",
      "Epoch [3830/10000], Loss: 3.0071024894714355\n",
      "Epoch [3840/10000], Loss: 2.984665870666504\n",
      "Epoch [3850/10000], Loss: 2.976715087890625\n",
      "Epoch [3860/10000], Loss: 2.9928903579711914\n",
      "Epoch [3870/10000], Loss: 2.994483232498169\n",
      "Epoch [3880/10000], Loss: 2.9990482330322266\n",
      "Epoch [3890/10000], Loss: 3.0013201236724854\n",
      "Epoch [3900/10000], Loss: 2.992722511291504\n",
      "Epoch [3910/10000], Loss: 2.9884750843048096\n",
      "Epoch [3920/10000], Loss: 2.999931812286377\n",
      "Epoch [3930/10000], Loss: 2.992415189743042\n",
      "Epoch [3940/10000], Loss: 2.9855968952178955\n",
      "Epoch [3950/10000], Loss: 2.9866247177124023\n",
      "Epoch [3960/10000], Loss: 2.9968080520629883\n",
      "Epoch [3970/10000], Loss: 2.999129295349121\n",
      "Epoch [3980/10000], Loss: 2.996542453765869\n",
      "Epoch [3990/10000], Loss: 2.9916038513183594\n",
      "Epoch [4000/10000], Loss: 2.9854207038879395\n",
      "Epoch [4010/10000], Loss: 2.9975063800811768\n",
      "Epoch [4020/10000], Loss: 2.984912395477295\n",
      "Epoch [4030/10000], Loss: 3.005056858062744\n",
      "Epoch [4040/10000], Loss: 2.997229814529419\n",
      "Epoch [4050/10000], Loss: 2.991150140762329\n",
      "Epoch [4060/10000], Loss: 2.988513469696045\n",
      "Epoch [4070/10000], Loss: 2.989994764328003\n",
      "Epoch [4080/10000], Loss: 2.9960789680480957\n",
      "Epoch [4090/10000], Loss: 2.9797511100769043\n",
      "Epoch [4100/10000], Loss: 2.9780566692352295\n",
      "Epoch [4110/10000], Loss: 2.9815421104431152\n",
      "Epoch [4120/10000], Loss: 2.9987430572509766\n",
      "Epoch [4130/10000], Loss: 2.9922609329223633\n",
      "Epoch [4140/10000], Loss: 2.989060401916504\n",
      "Epoch [4150/10000], Loss: 2.9963808059692383\n",
      "Epoch [4160/10000], Loss: 2.9818334579467773\n",
      "Epoch [4170/10000], Loss: 2.9995994567871094\n",
      "Epoch [4180/10000], Loss: 2.9912705421447754\n",
      "Epoch [4190/10000], Loss: 2.9831719398498535\n",
      "Epoch [4200/10000], Loss: 2.978335380554199\n",
      "Epoch [4210/10000], Loss: 2.990539073944092\n",
      "Epoch [4220/10000], Loss: 2.979759454727173\n",
      "Epoch [4230/10000], Loss: 2.9840595722198486\n",
      "Epoch [4240/10000], Loss: 2.9829962253570557\n",
      "Epoch [4250/10000], Loss: 2.9884448051452637\n",
      "Epoch [4260/10000], Loss: 2.986804723739624\n",
      "Epoch [4270/10000], Loss: 2.9846081733703613\n",
      "Epoch [4280/10000], Loss: 2.9895429611206055\n",
      "Epoch [4290/10000], Loss: 2.987596035003662\n",
      "Epoch [4300/10000], Loss: 2.995488166809082\n",
      "Epoch [4310/10000], Loss: 2.9828879833221436\n",
      "Epoch [4320/10000], Loss: 2.9703242778778076\n",
      "Epoch [4330/10000], Loss: 2.9987378120422363\n",
      "Epoch [4340/10000], Loss: 2.9952664375305176\n",
      "Epoch [4350/10000], Loss: 2.988025665283203\n",
      "Epoch [4360/10000], Loss: 2.9998655319213867\n",
      "Epoch [4370/10000], Loss: 2.9862899780273438\n",
      "Epoch [4380/10000], Loss: 2.982126235961914\n",
      "Epoch [4390/10000], Loss: 2.983294725418091\n",
      "Epoch [4400/10000], Loss: 2.9853954315185547\n",
      "Epoch [4410/10000], Loss: 2.9717092514038086\n",
      "Epoch [4420/10000], Loss: 2.985208749771118\n",
      "Epoch [4430/10000], Loss: 2.985996961593628\n",
      "Epoch [4440/10000], Loss: 2.982574224472046\n",
      "Epoch [4450/10000], Loss: 2.9893643856048584\n",
      "Epoch [4460/10000], Loss: 3.000516176223755\n",
      "Epoch [4470/10000], Loss: 2.9865846633911133\n",
      "Epoch [4480/10000], Loss: 2.98923659324646\n",
      "Epoch [4490/10000], Loss: 2.992159366607666\n",
      "Epoch [4500/10000], Loss: 2.9857125282287598\n",
      "Epoch [4510/10000], Loss: 2.980374574661255\n",
      "Epoch [4520/10000], Loss: 2.9933650493621826\n",
      "Epoch [4530/10000], Loss: 2.98732852935791\n",
      "Epoch [4540/10000], Loss: 2.9806408882141113\n",
      "Epoch [4550/10000], Loss: 2.9867000579833984\n",
      "Epoch [4560/10000], Loss: 2.9934234619140625\n",
      "Epoch [4570/10000], Loss: 3.0027308464050293\n",
      "Epoch [4580/10000], Loss: 2.983491897583008\n",
      "Epoch [4590/10000], Loss: 3.0013771057128906\n",
      "Epoch [4600/10000], Loss: 2.9888484477996826\n",
      "Epoch [4610/10000], Loss: 2.9900288581848145\n",
      "Epoch [4620/10000], Loss: 2.994636058807373\n",
      "Epoch [4630/10000], Loss: 2.9914848804473877\n",
      "Epoch [4640/10000], Loss: 2.9945149421691895\n",
      "Epoch [4650/10000], Loss: 2.9878346920013428\n",
      "Epoch [4660/10000], Loss: 2.983269453048706\n",
      "Epoch [4670/10000], Loss: 2.993380069732666\n",
      "Epoch [4680/10000], Loss: 3.00470232963562\n",
      "Epoch [4690/10000], Loss: 2.9905102252960205\n",
      "Epoch [4700/10000], Loss: 2.9951276779174805\n",
      "Epoch [4710/10000], Loss: 2.9914896488189697\n",
      "Epoch [4720/10000], Loss: 2.9785807132720947\n",
      "Epoch [4730/10000], Loss: 2.9856553077697754\n",
      "Epoch [4740/10000], Loss: 2.9894943237304688\n",
      "Epoch [4750/10000], Loss: 2.9869894981384277\n",
      "Epoch [4760/10000], Loss: 2.9863758087158203\n",
      "Epoch [4770/10000], Loss: 2.9814021587371826\n",
      "Epoch [4780/10000], Loss: 2.9904017448425293\n",
      "Epoch [4790/10000], Loss: 2.9855806827545166\n",
      "Epoch [4800/10000], Loss: 2.9945712089538574\n",
      "Epoch [4810/10000], Loss: 2.9908599853515625\n",
      "Epoch [4820/10000], Loss: 2.9837379455566406\n",
      "Epoch [4830/10000], Loss: 3.000088691711426\n",
      "Epoch [4840/10000], Loss: 2.9969754219055176\n",
      "Epoch [4850/10000], Loss: 2.988537549972534\n",
      "Epoch [4860/10000], Loss: 2.982705593109131\n",
      "Epoch [4870/10000], Loss: 2.997190237045288\n",
      "Epoch [4880/10000], Loss: 2.993413209915161\n",
      "Epoch [4890/10000], Loss: 2.9933879375457764\n",
      "Epoch [4900/10000], Loss: 3.001175880432129\n",
      "Epoch [4910/10000], Loss: 2.990902900695801\n",
      "Epoch [4920/10000], Loss: 2.9882593154907227\n",
      "Epoch [4930/10000], Loss: 2.9994380474090576\n",
      "Epoch [4940/10000], Loss: 2.98519229888916\n",
      "Epoch [4950/10000], Loss: 2.9827213287353516\n",
      "Epoch [4960/10000], Loss: 2.993565320968628\n",
      "Epoch [4970/10000], Loss: 2.9774880409240723\n",
      "Epoch [4980/10000], Loss: 2.9786288738250732\n",
      "Epoch [4990/10000], Loss: 2.9910402297973633\n",
      "Epoch [5000/10000], Loss: 2.9890122413635254\n",
      "Epoch [5010/10000], Loss: 2.9959535598754883\n",
      "Epoch [5020/10000], Loss: 2.9955782890319824\n",
      "Epoch [5030/10000], Loss: 2.9744439125061035\n",
      "Epoch [5040/10000], Loss: 2.9913554191589355\n",
      "Epoch [5050/10000], Loss: 2.9800362586975098\n",
      "Epoch [5060/10000], Loss: 3.0018224716186523\n",
      "Epoch [5070/10000], Loss: 2.9858508110046387\n",
      "Epoch [5080/10000], Loss: 2.9879095554351807\n",
      "Epoch [5090/10000], Loss: 2.9843358993530273\n",
      "Epoch [5100/10000], Loss: 2.98665189743042\n",
      "Epoch [5110/10000], Loss: 2.989471673965454\n",
      "Epoch [5120/10000], Loss: 3.002819299697876\n",
      "Epoch [5130/10000], Loss: 2.991436243057251\n",
      "Epoch [5140/10000], Loss: 2.9958438873291016\n",
      "Epoch [5150/10000], Loss: 2.991969585418701\n",
      "Epoch [5160/10000], Loss: 2.9953370094299316\n",
      "Epoch [5170/10000], Loss: 2.993828058242798\n",
      "Epoch [5180/10000], Loss: 2.9783387184143066\n",
      "Epoch [5190/10000], Loss: 2.9860382080078125\n",
      "Epoch [5200/10000], Loss: 2.977752685546875\n",
      "Epoch [5210/10000], Loss: 2.9982810020446777\n",
      "Epoch [5220/10000], Loss: 2.9920616149902344\n",
      "Epoch [5230/10000], Loss: 2.995669364929199\n",
      "Epoch [5240/10000], Loss: 2.989360809326172\n",
      "Epoch [5250/10000], Loss: 2.9936134815216064\n",
      "Epoch [5260/10000], Loss: 2.9838695526123047\n",
      "Epoch [5270/10000], Loss: 2.987877607345581\n",
      "Epoch [5280/10000], Loss: 2.999727487564087\n",
      "Epoch [5290/10000], Loss: 2.9974894523620605\n",
      "Epoch [5300/10000], Loss: 3.005621910095215\n",
      "Epoch [5310/10000], Loss: 2.9969024658203125\n",
      "Epoch [5320/10000], Loss: 2.990086793899536\n",
      "Epoch [5330/10000], Loss: 2.9934909343719482\n",
      "Epoch [5340/10000], Loss: 2.9831039905548096\n",
      "Epoch [5350/10000], Loss: 2.9772000312805176\n",
      "Epoch [5360/10000], Loss: 2.987844944000244\n",
      "Epoch [5370/10000], Loss: 2.989248275756836\n",
      "Epoch [5380/10000], Loss: 2.9778850078582764\n",
      "Epoch [5390/10000], Loss: 2.987420082092285\n",
      "Epoch [5400/10000], Loss: 2.982118606567383\n",
      "Epoch [5410/10000], Loss: 2.9934041500091553\n",
      "Epoch [5420/10000], Loss: 3.0104522705078125\n",
      "Epoch [5430/10000], Loss: 2.992023468017578\n",
      "Epoch [5440/10000], Loss: 3.0051815509796143\n",
      "Epoch [5450/10000], Loss: 3.0025792121887207\n",
      "Epoch [5460/10000], Loss: 2.996669292449951\n",
      "Epoch [5470/10000], Loss: 2.9822959899902344\n",
      "Epoch [5480/10000], Loss: 2.9828262329101562\n",
      "Epoch [5490/10000], Loss: 2.9825425148010254\n",
      "Epoch [5500/10000], Loss: 2.9858155250549316\n",
      "Epoch [5510/10000], Loss: 2.9875497817993164\n",
      "Epoch [5520/10000], Loss: 2.988894462585449\n",
      "Epoch [5530/10000], Loss: 2.9856691360473633\n",
      "Epoch [5540/10000], Loss: 2.9875268936157227\n",
      "Epoch [5550/10000], Loss: 2.986198663711548\n",
      "Epoch [5560/10000], Loss: 2.9930498600006104\n",
      "Epoch [5570/10000], Loss: 2.9889373779296875\n",
      "Epoch [5580/10000], Loss: 2.98026704788208\n",
      "Epoch [5590/10000], Loss: 2.9889445304870605\n",
      "Epoch [5600/10000], Loss: 2.9956843852996826\n",
      "Epoch [5610/10000], Loss: 2.983381509780884\n",
      "Epoch [5620/10000], Loss: 2.997964859008789\n",
      "Epoch [5630/10000], Loss: 2.9953789710998535\n",
      "Epoch [5640/10000], Loss: 2.985438346862793\n",
      "Epoch [5650/10000], Loss: 2.9799726009368896\n",
      "Epoch [5660/10000], Loss: 2.9874520301818848\n",
      "Epoch [5670/10000], Loss: 2.9922983646392822\n",
      "Epoch [5680/10000], Loss: 2.979654550552368\n",
      "Epoch [5690/10000], Loss: 2.985309600830078\n",
      "Epoch [5700/10000], Loss: 2.983241558074951\n",
      "Epoch [5710/10000], Loss: 2.9912660121917725\n",
      "Epoch [5720/10000], Loss: 2.992858409881592\n",
      "Epoch [5730/10000], Loss: 2.986887216567993\n",
      "Epoch [5740/10000], Loss: 2.9916000366210938\n",
      "Epoch [5750/10000], Loss: 2.9837541580200195\n",
      "Epoch [5760/10000], Loss: 2.9899044036865234\n",
      "Epoch [5770/10000], Loss: 2.9981658458709717\n",
      "Epoch [5780/10000], Loss: 2.9984045028686523\n",
      "Epoch [5790/10000], Loss: 2.983363628387451\n",
      "Epoch [5800/10000], Loss: 2.973741054534912\n",
      "Epoch [5810/10000], Loss: 2.9800031185150146\n",
      "Epoch [5820/10000], Loss: 2.993685722351074\n",
      "Epoch [5830/10000], Loss: 2.9904708862304688\n",
      "Epoch [5840/10000], Loss: 3.0024144649505615\n",
      "Epoch [5850/10000], Loss: 2.998394012451172\n",
      "Epoch [5860/10000], Loss: 2.9966518878936768\n",
      "Epoch [5870/10000], Loss: 2.9877090454101562\n",
      "Epoch [5880/10000], Loss: 2.9883759021759033\n",
      "Epoch [5890/10000], Loss: 3.0003609657287598\n",
      "Epoch [5900/10000], Loss: 2.998112916946411\n",
      "Epoch [5910/10000], Loss: 2.9851794242858887\n",
      "Epoch [5920/10000], Loss: 2.992901086807251\n",
      "Epoch [5930/10000], Loss: 2.9829046726226807\n",
      "Epoch [5940/10000], Loss: 2.9946374893188477\n",
      "Epoch [5950/10000], Loss: 2.9821174144744873\n",
      "Epoch [5960/10000], Loss: 2.9901037216186523\n",
      "Epoch [5970/10000], Loss: 2.9977405071258545\n",
      "Epoch [5980/10000], Loss: 2.9876370429992676\n",
      "Epoch [5990/10000], Loss: 2.9909679889678955\n",
      "Epoch [6000/10000], Loss: 2.9958009719848633\n",
      "Epoch [6010/10000], Loss: 2.976778745651245\n",
      "Epoch [6020/10000], Loss: 2.9685561656951904\n",
      "Epoch [6030/10000], Loss: 2.984280586242676\n",
      "Epoch [6040/10000], Loss: 2.9858694076538086\n",
      "Epoch [6050/10000], Loss: 2.9899721145629883\n",
      "Epoch [6060/10000], Loss: 2.9974944591522217\n",
      "Epoch [6070/10000], Loss: 2.992283582687378\n",
      "Epoch [6080/10000], Loss: 2.9860239028930664\n",
      "Epoch [6090/10000], Loss: 2.979706287384033\n",
      "Epoch [6100/10000], Loss: 2.9770166873931885\n",
      "Epoch [6110/10000], Loss: 2.978269338607788\n",
      "Epoch [6120/10000], Loss: 2.9946186542510986\n",
      "Epoch [6130/10000], Loss: 2.98366117477417\n",
      "Epoch [6140/10000], Loss: 2.98224139213562\n",
      "Epoch [6150/10000], Loss: 2.9863524436950684\n",
      "Epoch [6160/10000], Loss: 2.9874563217163086\n",
      "Epoch [6170/10000], Loss: 2.9782772064208984\n",
      "Epoch [6180/10000], Loss: 2.983247756958008\n",
      "Epoch [6190/10000], Loss: 2.983645439147949\n",
      "Epoch [6200/10000], Loss: 2.998141050338745\n",
      "Epoch [6210/10000], Loss: 2.9927566051483154\n",
      "Epoch [6220/10000], Loss: 2.9850544929504395\n",
      "Epoch [6230/10000], Loss: 2.992398738861084\n",
      "Epoch [6240/10000], Loss: 2.9915037155151367\n",
      "Epoch [6250/10000], Loss: 2.991628408432007\n",
      "Epoch [6260/10000], Loss: 2.9884519577026367\n",
      "Epoch [6270/10000], Loss: 2.98106050491333\n",
      "Epoch [6280/10000], Loss: 2.9902186393737793\n",
      "Epoch [6290/10000], Loss: 2.989441156387329\n",
      "Epoch [6300/10000], Loss: 2.9955787658691406\n",
      "Epoch [6310/10000], Loss: 2.9814491271972656\n",
      "Epoch [6320/10000], Loss: 2.9864604473114014\n",
      "Epoch [6330/10000], Loss: 2.9877889156341553\n",
      "Epoch [6340/10000], Loss: 2.9907195568084717\n",
      "Epoch [6350/10000], Loss: 2.9891164302825928\n",
      "Epoch [6360/10000], Loss: 2.9950833320617676\n",
      "Epoch [6370/10000], Loss: 2.988825798034668\n",
      "Epoch [6380/10000], Loss: 2.9686355590820312\n",
      "Epoch [6390/10000], Loss: 2.9804067611694336\n",
      "Epoch [6400/10000], Loss: 2.991380214691162\n",
      "Epoch [6410/10000], Loss: 2.9853086471557617\n",
      "Epoch [6420/10000], Loss: 2.9771058559417725\n",
      "Epoch [6430/10000], Loss: 3.0008723735809326\n",
      "Epoch [6440/10000], Loss: 2.9930338859558105\n",
      "Epoch [6450/10000], Loss: 2.9916129112243652\n",
      "Epoch [6460/10000], Loss: 2.987382411956787\n",
      "Epoch [6470/10000], Loss: 2.987076997756958\n",
      "Epoch [6480/10000], Loss: 2.9922051429748535\n",
      "Epoch [6490/10000], Loss: 2.979642629623413\n",
      "Epoch [6500/10000], Loss: 2.989870548248291\n",
      "Epoch [6510/10000], Loss: 2.976062297821045\n",
      "Epoch [6520/10000], Loss: 2.9946820735931396\n",
      "Epoch [6530/10000], Loss: 2.99179744720459\n",
      "Epoch [6540/10000], Loss: 2.998405933380127\n",
      "Epoch [6550/10000], Loss: 2.994795799255371\n",
      "Epoch [6560/10000], Loss: 3.0059397220611572\n",
      "Epoch [6570/10000], Loss: 2.9949913024902344\n",
      "Epoch [6580/10000], Loss: 2.983825445175171\n",
      "Epoch [6590/10000], Loss: 2.9957404136657715\n",
      "Epoch [6600/10000], Loss: 2.9912843704223633\n",
      "Epoch [6610/10000], Loss: 2.9873623847961426\n",
      "Epoch [6620/10000], Loss: 2.9867846965789795\n",
      "Epoch [6630/10000], Loss: 2.9933691024780273\n",
      "Epoch [6640/10000], Loss: 2.9793567657470703\n",
      "Epoch [6650/10000], Loss: 2.978407859802246\n",
      "Epoch [6660/10000], Loss: 2.985785484313965\n",
      "Epoch [6670/10000], Loss: 2.9968581199645996\n",
      "Epoch [6680/10000], Loss: 2.987114667892456\n",
      "Epoch [6690/10000], Loss: 2.9995474815368652\n",
      "Epoch [6700/10000], Loss: 2.998931407928467\n",
      "Epoch [6710/10000], Loss: 2.9918720722198486\n",
      "Epoch [6720/10000], Loss: 2.9999923706054688\n",
      "Epoch [6730/10000], Loss: 2.999293804168701\n",
      "Epoch [6740/10000], Loss: 2.988546133041382\n",
      "Epoch [6750/10000], Loss: 2.9891254901885986\n",
      "Epoch [6760/10000], Loss: 2.99025297164917\n",
      "Epoch [6770/10000], Loss: 2.9776790142059326\n",
      "Epoch [6780/10000], Loss: 2.988722324371338\n",
      "Epoch [6790/10000], Loss: 2.9930286407470703\n",
      "Epoch [6800/10000], Loss: 2.993802309036255\n",
      "Epoch [6810/10000], Loss: 2.9884016513824463\n",
      "Epoch [6820/10000], Loss: 2.97824764251709\n",
      "Epoch [6830/10000], Loss: 2.9937896728515625\n",
      "Epoch [6840/10000], Loss: 2.9872090816497803\n",
      "Epoch [6850/10000], Loss: 2.9725940227508545\n",
      "Epoch [6860/10000], Loss: 2.9822874069213867\n",
      "Epoch [6870/10000], Loss: 2.9876255989074707\n",
      "Epoch [6880/10000], Loss: 2.9862539768218994\n",
      "Epoch [6890/10000], Loss: 2.992008686065674\n",
      "Epoch [6900/10000], Loss: 2.9856085777282715\n",
      "Epoch [6910/10000], Loss: 2.994790554046631\n",
      "Epoch [6920/10000], Loss: 2.9864368438720703\n",
      "Epoch [6930/10000], Loss: 2.9877023696899414\n",
      "Epoch [6940/10000], Loss: 2.98931622505188\n",
      "Epoch [6950/10000], Loss: 2.987192392349243\n",
      "Epoch [6960/10000], Loss: 2.981527805328369\n",
      "Epoch [6970/10000], Loss: 2.992279291152954\n",
      "Epoch [6980/10000], Loss: 2.9987411499023438\n",
      "Epoch [6990/10000], Loss: 3.0061466693878174\n",
      "Epoch [7000/10000], Loss: 2.9859139919281006\n",
      "Epoch [7010/10000], Loss: 2.9870657920837402\n",
      "Epoch [7020/10000], Loss: 2.998399496078491\n",
      "Epoch [7030/10000], Loss: 2.994178056716919\n",
      "Epoch [7040/10000], Loss: 2.989675521850586\n",
      "Epoch [7050/10000], Loss: 2.9937732219696045\n",
      "Epoch [7060/10000], Loss: 2.998514413833618\n",
      "Epoch [7070/10000], Loss: 2.995122194290161\n",
      "Epoch [7080/10000], Loss: 3.0040977001190186\n",
      "Epoch [7090/10000], Loss: 2.9971725940704346\n",
      "Epoch [7100/10000], Loss: 2.9981722831726074\n",
      "Epoch [7110/10000], Loss: 3.0001728534698486\n",
      "Epoch [7120/10000], Loss: 2.988945722579956\n",
      "Epoch [7130/10000], Loss: 2.9859559535980225\n",
      "Epoch [7140/10000], Loss: 2.9920854568481445\n",
      "Epoch [7150/10000], Loss: 2.979590892791748\n",
      "Epoch [7160/10000], Loss: 2.9801154136657715\n",
      "Epoch [7170/10000], Loss: 2.9772794246673584\n",
      "Epoch [7180/10000], Loss: 2.990128755569458\n",
      "Epoch [7190/10000], Loss: 2.992441415786743\n",
      "Epoch [7200/10000], Loss: 2.9856433868408203\n",
      "Epoch [7210/10000], Loss: 2.982905387878418\n",
      "Epoch [7220/10000], Loss: 2.987161636352539\n",
      "Epoch [7230/10000], Loss: 2.9977102279663086\n",
      "Epoch [7240/10000], Loss: 2.9925825595855713\n",
      "Epoch [7250/10000], Loss: 2.9906973838806152\n",
      "Epoch [7260/10000], Loss: 2.987992525100708\n",
      "Epoch [7270/10000], Loss: 2.986945152282715\n",
      "Epoch [7280/10000], Loss: 2.979001522064209\n",
      "Epoch [7290/10000], Loss: 2.9917092323303223\n",
      "Epoch [7300/10000], Loss: 2.989541530609131\n",
      "Epoch [7310/10000], Loss: 2.9849700927734375\n",
      "Epoch [7320/10000], Loss: 2.983440399169922\n",
      "Epoch [7330/10000], Loss: 2.9889402389526367\n",
      "Epoch [7340/10000], Loss: 2.980147123336792\n",
      "Epoch [7350/10000], Loss: 2.9844675064086914\n",
      "Epoch [7360/10000], Loss: 2.985995292663574\n",
      "Epoch [7370/10000], Loss: 2.9840757846832275\n",
      "Epoch [7380/10000], Loss: 2.990981101989746\n",
      "Epoch [7390/10000], Loss: 2.985347032546997\n",
      "Epoch [7400/10000], Loss: 2.9810731410980225\n",
      "Epoch [7410/10000], Loss: 2.987609386444092\n",
      "Epoch [7420/10000], Loss: 2.985903024673462\n",
      "Epoch [7430/10000], Loss: 2.989248275756836\n",
      "Epoch [7440/10000], Loss: 2.9862523078918457\n",
      "Epoch [7450/10000], Loss: 2.988100528717041\n",
      "Epoch [7460/10000], Loss: 2.996303081512451\n",
      "Epoch [7470/10000], Loss: 3.0078535079956055\n",
      "Epoch [7480/10000], Loss: 2.992060661315918\n",
      "Epoch [7490/10000], Loss: 2.990769386291504\n",
      "Epoch [7500/10000], Loss: 2.9976425170898438\n",
      "Epoch [7510/10000], Loss: 2.9816031455993652\n",
      "Epoch [7520/10000], Loss: 2.9901065826416016\n",
      "Epoch [7530/10000], Loss: 2.9931788444519043\n",
      "Epoch [7540/10000], Loss: 2.981466770172119\n",
      "Epoch [7550/10000], Loss: 2.989419460296631\n",
      "Epoch [7560/10000], Loss: 2.990216016769409\n",
      "Epoch [7570/10000], Loss: 2.9770138263702393\n",
      "Epoch [7580/10000], Loss: 2.990459680557251\n",
      "Epoch [7590/10000], Loss: 2.996171236038208\n",
      "Epoch [7600/10000], Loss: 2.99885892868042\n",
      "Epoch [7610/10000], Loss: 2.987388849258423\n",
      "Epoch [7620/10000], Loss: 2.982445001602173\n",
      "Epoch [7630/10000], Loss: 2.990882635116577\n",
      "Epoch [7640/10000], Loss: 2.982834577560425\n",
      "Epoch [7650/10000], Loss: 2.995912551879883\n",
      "Epoch [7660/10000], Loss: 3.0034189224243164\n",
      "Epoch [7670/10000], Loss: 2.981339931488037\n",
      "Epoch [7680/10000], Loss: 2.9828383922576904\n",
      "Epoch [7690/10000], Loss: 2.9834747314453125\n",
      "Epoch [7700/10000], Loss: 3.0007617473602295\n",
      "Epoch [7710/10000], Loss: 3.002336025238037\n",
      "Epoch [7720/10000], Loss: 2.99460506439209\n",
      "Epoch [7730/10000], Loss: 2.9914908409118652\n",
      "Epoch [7740/10000], Loss: 2.9839577674865723\n",
      "Epoch [7750/10000], Loss: 2.98679256439209\n",
      "Epoch [7760/10000], Loss: 2.9936976432800293\n",
      "Epoch [7770/10000], Loss: 2.9953479766845703\n",
      "Epoch [7780/10000], Loss: 2.9891276359558105\n",
      "Epoch [7790/10000], Loss: 2.994692802429199\n",
      "Epoch [7800/10000], Loss: 2.997159481048584\n",
      "Epoch [7810/10000], Loss: 2.9888148307800293\n",
      "Epoch [7820/10000], Loss: 3.0060393810272217\n",
      "Epoch [7830/10000], Loss: 2.981145143508911\n",
      "Epoch [7840/10000], Loss: 2.9862585067749023\n",
      "Epoch [7850/10000], Loss: 2.996868371963501\n",
      "Epoch [7860/10000], Loss: 2.9906251430511475\n",
      "Epoch [7870/10000], Loss: 3.008728265762329\n",
      "Epoch [7880/10000], Loss: 2.993368148803711\n",
      "Epoch [7890/10000], Loss: 2.9851531982421875\n",
      "Epoch [7900/10000], Loss: 2.990182876586914\n",
      "Epoch [7910/10000], Loss: 2.9910645484924316\n",
      "Epoch [7920/10000], Loss: 2.983734607696533\n",
      "Epoch [7930/10000], Loss: 2.986626625061035\n",
      "Epoch [7940/10000], Loss: 2.9819138050079346\n",
      "Epoch [7950/10000], Loss: 2.9899961948394775\n",
      "Epoch [7960/10000], Loss: 2.997617721557617\n",
      "Epoch [7970/10000], Loss: 2.9975271224975586\n",
      "Epoch [7980/10000], Loss: 2.997621774673462\n",
      "Epoch [7990/10000], Loss: 2.9922802448272705\n",
      "Epoch [8000/10000], Loss: 2.9853157997131348\n",
      "Epoch [8010/10000], Loss: 2.978834867477417\n",
      "Epoch [8020/10000], Loss: 2.979743480682373\n",
      "Epoch [8030/10000], Loss: 2.9951813220977783\n",
      "Epoch [8040/10000], Loss: 2.986743927001953\n",
      "Epoch [8050/10000], Loss: 2.981001377105713\n",
      "Epoch [8060/10000], Loss: 2.9762179851531982\n",
      "Epoch [8070/10000], Loss: 2.9795477390289307\n",
      "Epoch [8080/10000], Loss: 2.976546049118042\n",
      "Epoch [8090/10000], Loss: 2.99005126953125\n",
      "Epoch [8100/10000], Loss: 2.9933078289031982\n",
      "Epoch [8110/10000], Loss: 3.000279664993286\n",
      "Epoch [8120/10000], Loss: 2.9764130115509033\n",
      "Epoch [8130/10000], Loss: 2.989016532897949\n",
      "Epoch [8140/10000], Loss: 2.9889562129974365\n",
      "Epoch [8150/10000], Loss: 2.9959306716918945\n",
      "Epoch [8160/10000], Loss: 2.989016532897949\n",
      "Epoch [8170/10000], Loss: 2.986677408218384\n",
      "Epoch [8180/10000], Loss: 2.982943534851074\n",
      "Epoch [8190/10000], Loss: 2.9920501708984375\n",
      "Epoch [8200/10000], Loss: 2.992201805114746\n",
      "Epoch [8210/10000], Loss: 2.9836959838867188\n",
      "Epoch [8220/10000], Loss: 2.992298126220703\n",
      "Epoch [8230/10000], Loss: 2.985358953475952\n",
      "Epoch [8240/10000], Loss: 2.985574722290039\n",
      "Epoch [8250/10000], Loss: 3.0018208026885986\n",
      "Epoch [8260/10000], Loss: 2.9894859790802\n",
      "Epoch [8270/10000], Loss: 2.9935550689697266\n",
      "Epoch [8280/10000], Loss: 2.990797996520996\n",
      "Epoch [8290/10000], Loss: 2.992526054382324\n",
      "Epoch [8300/10000], Loss: 2.984273672103882\n",
      "Epoch [8310/10000], Loss: 3.0052542686462402\n",
      "Epoch [8320/10000], Loss: 3.0014986991882324\n",
      "Epoch [8330/10000], Loss: 2.984513282775879\n",
      "Epoch [8340/10000], Loss: 2.992842435836792\n",
      "Epoch [8350/10000], Loss: 2.9989848136901855\n",
      "Epoch [8360/10000], Loss: 2.989420175552368\n",
      "Epoch [8370/10000], Loss: 2.982428789138794\n",
      "Epoch [8380/10000], Loss: 2.99230694770813\n",
      "Epoch [8390/10000], Loss: 2.985917091369629\n",
      "Epoch [8400/10000], Loss: 2.9844226837158203\n",
      "Epoch [8410/10000], Loss: 2.9865777492523193\n",
      "Epoch [8420/10000], Loss: 2.9975202083587646\n",
      "Epoch [8430/10000], Loss: 2.9908275604248047\n",
      "Epoch [8440/10000], Loss: 2.9867687225341797\n",
      "Epoch [8450/10000], Loss: 2.983525514602661\n",
      "Epoch [8460/10000], Loss: 2.986649990081787\n",
      "Epoch [8470/10000], Loss: 2.9908995628356934\n",
      "Epoch [8480/10000], Loss: 2.982856035232544\n",
      "Epoch [8490/10000], Loss: 2.991621494293213\n",
      "Epoch [8500/10000], Loss: 2.982649803161621\n",
      "Epoch [8510/10000], Loss: 2.9886374473571777\n",
      "Epoch [8520/10000], Loss: 2.974660634994507\n",
      "Epoch [8530/10000], Loss: 2.9846715927124023\n",
      "Epoch [8540/10000], Loss: 2.9866080284118652\n",
      "Epoch [8550/10000], Loss: 2.9866650104522705\n",
      "Epoch [8560/10000], Loss: 2.989635944366455\n",
      "Epoch [8570/10000], Loss: 2.9952566623687744\n",
      "Epoch [8580/10000], Loss: 2.982729911804199\n",
      "Epoch [8590/10000], Loss: 2.978241205215454\n",
      "Epoch [8600/10000], Loss: 2.990168571472168\n",
      "Epoch [8610/10000], Loss: 2.9868671894073486\n",
      "Epoch [8620/10000], Loss: 2.9894561767578125\n",
      "Epoch [8630/10000], Loss: 2.975686550140381\n",
      "Epoch [8640/10000], Loss: 2.984377861022949\n",
      "Epoch [8650/10000], Loss: 2.9985909461975098\n",
      "Epoch [8660/10000], Loss: 2.9897844791412354\n",
      "Epoch [8670/10000], Loss: 2.9922804832458496\n",
      "Epoch [8680/10000], Loss: 2.987558364868164\n",
      "Epoch [8690/10000], Loss: 2.9799230098724365\n",
      "Epoch [8700/10000], Loss: 2.977522850036621\n",
      "Epoch [8710/10000], Loss: 2.9838645458221436\n",
      "Epoch [8720/10000], Loss: 2.984900951385498\n",
      "Epoch [8730/10000], Loss: 2.9906811714172363\n",
      "Epoch [8740/10000], Loss: 3.002229690551758\n",
      "Epoch [8750/10000], Loss: 2.9829483032226562\n",
      "Epoch [8760/10000], Loss: 2.987569570541382\n",
      "Epoch [8770/10000], Loss: 2.994716167449951\n",
      "Epoch [8780/10000], Loss: 2.9879579544067383\n",
      "Epoch [8790/10000], Loss: 2.990180492401123\n",
      "Epoch [8800/10000], Loss: 2.99133038520813\n",
      "Epoch [8810/10000], Loss: 2.9890575408935547\n",
      "Epoch [8820/10000], Loss: 2.9949636459350586\n",
      "Epoch [8830/10000], Loss: 2.999260425567627\n",
      "Epoch [8840/10000], Loss: 2.9847664833068848\n",
      "Epoch [8850/10000], Loss: 2.990049362182617\n",
      "Epoch [8860/10000], Loss: 2.9878411293029785\n",
      "Epoch [8870/10000], Loss: 2.979123592376709\n",
      "Epoch [8880/10000], Loss: 2.9751758575439453\n",
      "Epoch [8890/10000], Loss: 2.975006580352783\n",
      "Epoch [8900/10000], Loss: 2.981532573699951\n",
      "Epoch [8910/10000], Loss: 2.980165958404541\n",
      "Epoch [8920/10000], Loss: 2.9890925884246826\n",
      "Epoch [8930/10000], Loss: 2.982091188430786\n",
      "Epoch [8940/10000], Loss: 3.000272035598755\n",
      "Epoch [8950/10000], Loss: 3.003164291381836\n",
      "Epoch [8960/10000], Loss: 2.994570732116699\n",
      "Epoch [8970/10000], Loss: 2.9981842041015625\n",
      "Epoch [8980/10000], Loss: 2.9944047927856445\n",
      "Epoch [8990/10000], Loss: 3.0028185844421387\n",
      "Epoch [9000/10000], Loss: 2.996077060699463\n",
      "Epoch [9010/10000], Loss: 2.9886722564697266\n",
      "Epoch [9020/10000], Loss: 2.992591381072998\n",
      "Epoch [9030/10000], Loss: 2.9880177974700928\n",
      "Epoch [9040/10000], Loss: 2.996150016784668\n",
      "Epoch [9050/10000], Loss: 2.9916296005249023\n",
      "Epoch [9060/10000], Loss: 2.991896152496338\n",
      "Epoch [9070/10000], Loss: 2.9932875633239746\n",
      "Epoch [9080/10000], Loss: 2.992502212524414\n",
      "Epoch [9090/10000], Loss: 2.993454933166504\n",
      "Epoch [9100/10000], Loss: 2.987499952316284\n",
      "Epoch [9110/10000], Loss: 2.988426446914673\n",
      "Epoch [9120/10000], Loss: 2.9925003051757812\n",
      "Epoch [9130/10000], Loss: 2.9968101978302\n",
      "Epoch [9140/10000], Loss: 3.0091896057128906\n",
      "Epoch [9150/10000], Loss: 2.9999008178710938\n",
      "Epoch [9160/10000], Loss: 2.9839730262756348\n",
      "Epoch [9170/10000], Loss: 2.981414556503296\n",
      "Epoch [9180/10000], Loss: 2.9645681381225586\n",
      "Epoch [9190/10000], Loss: 2.9841253757476807\n",
      "Epoch [9200/10000], Loss: 2.9824366569519043\n",
      "Epoch [9210/10000], Loss: 2.984557628631592\n",
      "Epoch [9220/10000], Loss: 2.9900379180908203\n",
      "Epoch [9230/10000], Loss: 2.993009328842163\n",
      "Epoch [9240/10000], Loss: 2.987839698791504\n",
      "Epoch [9250/10000], Loss: 2.9878735542297363\n",
      "Epoch [9260/10000], Loss: 2.9852867126464844\n",
      "Epoch [9270/10000], Loss: 2.9928181171417236\n",
      "Epoch [9280/10000], Loss: 2.9725375175476074\n",
      "Epoch [9290/10000], Loss: 2.9963512420654297\n",
      "Epoch [9300/10000], Loss: 2.9941442012786865\n",
      "Epoch [9310/10000], Loss: 2.9895734786987305\n",
      "Epoch [9320/10000], Loss: 2.986579418182373\n",
      "Epoch [9330/10000], Loss: 2.9854423999786377\n",
      "Epoch [9340/10000], Loss: 2.994988441467285\n",
      "Epoch [9350/10000], Loss: 2.994793653488159\n",
      "Epoch [9360/10000], Loss: 2.983048915863037\n",
      "Epoch [9370/10000], Loss: 2.994288444519043\n",
      "Epoch [9380/10000], Loss: 3.0062954425811768\n",
      "Epoch [9390/10000], Loss: 3.0002851486206055\n",
      "Epoch [9400/10000], Loss: 2.998680591583252\n",
      "Epoch [9410/10000], Loss: 2.9799211025238037\n",
      "Epoch [9420/10000], Loss: 2.9848673343658447\n",
      "Epoch [9430/10000], Loss: 2.9784741401672363\n",
      "Epoch [9440/10000], Loss: 2.9676108360290527\n",
      "Epoch [9450/10000], Loss: 2.9714150428771973\n",
      "Epoch [9460/10000], Loss: 2.976682186126709\n",
      "Epoch [9470/10000], Loss: 2.988950490951538\n",
      "Epoch [9480/10000], Loss: 3.0032846927642822\n",
      "Epoch [9490/10000], Loss: 2.9894859790802\n",
      "Epoch [9500/10000], Loss: 2.9868111610412598\n",
      "Epoch [9510/10000], Loss: 2.9801933765411377\n",
      "Epoch [9520/10000], Loss: 2.990290641784668\n",
      "Epoch [9530/10000], Loss: 2.9989981651306152\n",
      "Epoch [9540/10000], Loss: 2.996644973754883\n",
      "Epoch [9550/10000], Loss: 2.9868428707122803\n",
      "Epoch [9560/10000], Loss: 2.983814239501953\n",
      "Epoch [9570/10000], Loss: 2.981517791748047\n",
      "Epoch [9580/10000], Loss: 2.9861273765563965\n",
      "Epoch [9590/10000], Loss: 2.9865353107452393\n",
      "Epoch [9600/10000], Loss: 2.983187198638916\n",
      "Epoch [9610/10000], Loss: 2.9982335567474365\n",
      "Epoch [9620/10000], Loss: 2.976712942123413\n",
      "Epoch [9630/10000], Loss: 2.9935848712921143\n",
      "Epoch [9640/10000], Loss: 2.9913625717163086\n",
      "Epoch [9650/10000], Loss: 2.990640640258789\n",
      "Epoch [9660/10000], Loss: 2.989421844482422\n",
      "Epoch [9670/10000], Loss: 2.987356185913086\n",
      "Epoch [9680/10000], Loss: 2.9838907718658447\n",
      "Epoch [9690/10000], Loss: 2.9913883209228516\n",
      "Epoch [9700/10000], Loss: 2.995530843734741\n",
      "Epoch [9710/10000], Loss: 2.9868040084838867\n",
      "Epoch [9720/10000], Loss: 2.987074613571167\n",
      "Epoch [9730/10000], Loss: 2.9934964179992676\n",
      "Epoch [9740/10000], Loss: 2.978760242462158\n",
      "Epoch [9750/10000], Loss: 2.987607717514038\n",
      "Epoch [9760/10000], Loss: 2.976266384124756\n",
      "Epoch [9770/10000], Loss: 2.9806969165802\n",
      "Epoch [9780/10000], Loss: 2.984266996383667\n",
      "Epoch [9790/10000], Loss: 2.9912965297698975\n",
      "Epoch [9800/10000], Loss: 2.982664108276367\n",
      "Epoch [9810/10000], Loss: 2.9884519577026367\n",
      "Epoch [9820/10000], Loss: 2.980473279953003\n",
      "Epoch [9830/10000], Loss: 2.9779577255249023\n",
      "Epoch [9840/10000], Loss: 2.983690023422241\n",
      "Epoch [9850/10000], Loss: 2.9878056049346924\n",
      "Epoch [9860/10000], Loss: 2.979665756225586\n",
      "Epoch [9870/10000], Loss: 2.9893667697906494\n",
      "Epoch [9880/10000], Loss: 2.9914939403533936\n",
      "Epoch [9890/10000], Loss: 2.9887280464172363\n",
      "Epoch [9900/10000], Loss: 2.9814131259918213\n",
      "Epoch [9910/10000], Loss: 2.988924741744995\n",
      "Epoch [9920/10000], Loss: 2.9859402179718018\n",
      "Epoch [9930/10000], Loss: 2.9867377281188965\n",
      "Epoch [9940/10000], Loss: 2.9932820796966553\n",
      "Epoch [9950/10000], Loss: 2.987823009490967\n",
      "Epoch [9960/10000], Loss: 2.9824817180633545\n",
      "Epoch [9970/10000], Loss: 2.993222236633301\n",
      "Epoch [9980/10000], Loss: 2.9823436737060547\n",
      "Epoch [9990/10000], Loss: 2.9845526218414307\n",
      "Epoch [10000/10000], Loss: 2.9973318576812744\n",
      "Epoch [100/10000], Loss: 3.1398053500772223\n",
      "Epoch [200/10000], Loss: 2.883821845789498\n",
      "Epoch [300/10000], Loss: 2.7015012409545305\n",
      "Epoch [400/10000], Loss: 2.592159482230636\n",
      "Epoch [500/10000], Loss: 2.533382285104017\n",
      "Epoch [600/10000], Loss: 2.4939571502000035\n",
      "Epoch [700/10000], Loss: 2.4688326951727504\n",
      "Epoch [800/10000], Loss: 2.4577384381442244\n",
      "Epoch [900/10000], Loss: 2.452882119672722\n",
      "Epoch [1000/10000], Loss: 2.449921762906888\n",
      "Epoch [1100/10000], Loss: 2.451579566753935\n",
      "Epoch [1200/10000], Loss: 2.4480800631674358\n",
      "Epoch [1300/10000], Loss: 2.4493135091230216\n",
      "Epoch [1400/10000], Loss: 2.4490122941702794\n",
      "Epoch [1500/10000], Loss: 2.446894914121367\n",
      "Epoch [1600/10000], Loss: 2.4476193595284714\n",
      "Epoch [1700/10000], Loss: 2.4503421933812204\n",
      "Epoch [1800/10000], Loss: 2.4486542158105293\n",
      "Epoch [1900/10000], Loss: 2.4473759479289585\n",
      "Epoch [2000/10000], Loss: 2.4489977062869004\n",
      "Epoch [2100/10000], Loss: 2.44994198621207\n",
      "Epoch [2200/10000], Loss: 2.4510289601552357\n",
      "Epoch [2300/10000], Loss: 2.4483528324170036\n",
      "Epoch [2400/10000], Loss: 2.4476119504819507\n",
      "Epoch [2500/10000], Loss: 2.448810760160268\n",
      "Epoch [2600/10000], Loss: 2.4483687110128813\n",
      "Epoch [2700/10000], Loss: 2.4503704046961503\n",
      "Epoch [2800/10000], Loss: 2.448578576820728\n",
      "Epoch [2900/10000], Loss: 2.447401245585934\n",
      "Epoch [3000/10000], Loss: 2.449072991636058\n",
      "Epoch [3100/10000], Loss: 2.450399059988558\n",
      "Epoch [3200/10000], Loss: 2.4497642622431157\n",
      "Epoch [3300/10000], Loss: 2.4474657744136494\n",
      "Epoch [3400/10000], Loss: 2.448957762376813\n",
      "Epoch [3500/10000], Loss: 2.450256016437925\n",
      "Epoch [3600/10000], Loss: 2.448578303416434\n",
      "Epoch [3700/10000], Loss: 2.4481103748046733\n",
      "Epoch [3800/10000], Loss: 2.4477331458685514\n",
      "Epoch [3900/10000], Loss: 2.4490350769629003\n",
      "Epoch [4000/10000], Loss: 2.447443176269735\n",
      "Epoch [4100/10000], Loss: 2.447980987819028\n",
      "Epoch [4200/10000], Loss: 2.4516096220933834\n",
      "Epoch [4300/10000], Loss: 2.4472173047244725\n",
      "Epoch [4400/10000], Loss: 2.4497787051332125\n",
      "Epoch [4500/10000], Loss: 2.4506780786497986\n",
      "Epoch [4600/10000], Loss: 2.4473354954803654\n",
      "Epoch [4700/10000], Loss: 2.4466779439448145\n",
      "Epoch [4800/10000], Loss: 2.4496368394196906\n",
      "Epoch [4900/10000], Loss: 2.449733863440997\n",
      "Epoch [5000/10000], Loss: 2.4489763628822403\n",
      "Epoch [5100/10000], Loss: 2.4487607461484004\n",
      "Epoch [5200/10000], Loss: 2.450402504953672\n",
      "Epoch [5300/10000], Loss: 2.449275869686062\n",
      "Epoch [5400/10000], Loss: 2.4479954672788153\n",
      "Epoch [5500/10000], Loss: 2.450237498313072\n",
      "Epoch [5600/10000], Loss: 2.4505811778675706\n",
      "Epoch [5700/10000], Loss: 2.447733835409599\n",
      "Epoch [5800/10000], Loss: 2.4482273034780517\n",
      "Epoch [5900/10000], Loss: 2.448584326959099\n",
      "Epoch [6000/10000], Loss: 2.447874758645594\n",
      "Epoch [6100/10000], Loss: 2.4509976965811804\n",
      "Epoch [6200/10000], Loss: 2.4485740790798447\n",
      "Epoch [6300/10000], Loss: 2.4494282411411405\n",
      "Epoch [6400/10000], Loss: 2.4477508049007155\n",
      "Epoch [6500/10000], Loss: 2.4459683489896635\n",
      "Epoch [6600/10000], Loss: 2.449027990068134\n",
      "Epoch [6700/10000], Loss: 2.4496445785582184\n",
      "Epoch [6800/10000], Loss: 2.4498630308618887\n",
      "Epoch [6900/10000], Loss: 2.451042781674187\n",
      "Epoch [7000/10000], Loss: 2.447802371871512\n",
      "Epoch [7100/10000], Loss: 2.44754946056928\n",
      "Epoch [7200/10000], Loss: 2.450827238571219\n",
      "Epoch [7300/10000], Loss: 2.448831099322342\n",
      "Epoch [7400/10000], Loss: 2.45141057719884\n",
      "Epoch [7500/10000], Loss: 2.450243463082006\n",
      "Epoch [7600/10000], Loss: 2.4487734376234584\n",
      "Epoch [7700/10000], Loss: 2.4490815819212\n",
      "Epoch [7800/10000], Loss: 2.450093624402143\n",
      "Epoch [7900/10000], Loss: 2.4493162591810687\n",
      "Epoch [8000/10000], Loss: 2.4471592715141015\n",
      "Epoch [8100/10000], Loss: 2.449470323366404\n",
      "Epoch [8200/10000], Loss: 2.447235251298116\n",
      "Epoch [8300/10000], Loss: 2.4477723591800897\n",
      "Epoch [8400/10000], Loss: 2.4505815058895677\n",
      "Epoch [8500/10000], Loss: 2.447568147797574\n",
      "Epoch [8600/10000], Loss: 2.4485369672620436\n",
      "Epoch [8700/10000], Loss: 2.4498267533883338\n",
      "Epoch [8800/10000], Loss: 2.447676629164198\n",
      "Epoch [8900/10000], Loss: 2.4507915926427812\n",
      "Epoch [9000/10000], Loss: 2.4498042257037014\n",
      "Epoch [9100/10000], Loss: 2.448126750110896\n",
      "Epoch [9200/10000], Loss: 2.44922474205232\n",
      "Epoch [9300/10000], Loss: 2.4521620901083225\n",
      "Epoch [9400/10000], Loss: 2.4477187556149147\n",
      "Epoch [9500/10000], Loss: 2.450587392473244\n",
      "Epoch [9600/10000], Loss: 2.4497083109919915\n",
      "Epoch [9700/10000], Loss: 2.4481775132226176\n",
      "Epoch [9800/10000], Loss: 2.4495474134833786\n",
      "Epoch [9900/10000], Loss: 2.4476080278706833\n",
      "Epoch [10000/10000], Loss: 2.4468426382984036\n"
     ]
    }
   ],
   "source": [
    "enet_model = ElasticNet(input_size=x_torch.shape[1],alpha=0.1,l1_ratio=0.5)\n",
    "scad_model = SCAD(input_size=x_torch.shape[1],alpha=0.1,lmda=0.5)\n",
    "sqrtlas_model = SqrtLasso(input_size=x_torch.shape[1],alpha=0.1)\n",
    "\n",
    "enet_model.fit(x_torch,y_torch, num_epochs=10000, learning_rate=0.01)\n",
    "scad_model.fit(x_torch,y_torch, num_epochs=10000, learning_rate=0.01)\n",
    "sqrtlas_model.fit(x_torch,y_torch, num_epochs=10000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_pred = enet_model.predict(x_torch)\n",
    "scad_pred = scad_model.predict(x_torch)\n",
    "sqrtlas_pred = sqrtlas_model.predict(x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.329589279941096, 2.30380426512322, 2.3230125629204164)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(enet_pred, y), mse(scad_pred, y), mse(sqrtlas_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that SCAD regularization produces the best approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 3\n",
    "Host your project on your GitHub page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "https://github.com/Pschnizer/DATA441/blob/main/DATA_441_Project_3.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
